[
  {
    "objectID": "Workbooks/wb2.html",
    "href": "Workbooks/wb2.html",
    "title": "Project 2 Workbook",
    "section": "",
    "text": "The data science lab is a resource you can use in person, online, and in Slack.",
    "crumbs": [
      "Workbooks",
      "Project 2"
    ]
  },
  {
    "objectID": "Workbooks/wb2.html#tutoring-lab-info",
    "href": "Workbooks/wb2.html#tutoring-lab-info",
    "title": "Project 2 Workbook",
    "section": "",
    "text": "The data science lab is a resource you can use in person, online, and in Slack.",
    "crumbs": [
      "Workbooks",
      "Project 2"
    ]
  },
  {
    "objectID": "Workbooks/wb2.html#plotly-chart-structure",
    "href": "Workbooks/wb2.html#plotly-chart-structure",
    "title": "Project 2 Workbook",
    "section": "Plotly Chart Structure",
    "text": "Plotly Chart Structure\n\n\nShow the code\nfig = px.scatter(data, x='displ', y='hwy')\nfig.show()",
    "crumbs": [
      "Workbooks",
      "Project 2"
    ]
  },
  {
    "objectID": "Workbooks/wb2.html#size-of-chart",
    "href": "Workbooks/wb2.html#size-of-chart",
    "title": "Project 2 Workbook",
    "section": "Size of Chart",
    "text": "Size of Chart\nWidth and Height\n\n\nShow the code\nfig = px.scatter(data, x='displ', y='hwy')\nfig.update_layout(width=600, height=600)\n\nfig.show()\n\n\n                                                \n\n\n\n\n\n\n\n\nWidth and Height Explanation\n\n\n\n\n\nThe width and height parameters in the update_layout method are used to set the width and height of the plot in pixels, respectively. In this example, the width is set to 600 pixels, and the height is also set to 600 pixels. Adjust these values according to your desired dimensions for the scatter plot.",
    "crumbs": [
      "Workbooks",
      "Project 2"
    ]
  },
  {
    "objectID": "Workbooks/wb2.html#title-and-subtitle",
    "href": "Workbooks/wb2.html#title-and-subtitle",
    "title": "Project 2 Workbook",
    "section": "Title and Subtitle",
    "text": "Title and Subtitle\nTitle\n\n\nShow the code\nfig = px.bar(data, x='cty', y='hwy')\nfig.update_layout(title_text=\"Bar Chart Example\")\nfig.show()\n\n\n                                                \n\n\n\n\n\n\n\n\nTitle Explanation\n\n\n\n\n\nThe title_text parameter in the update_layout method is used to set the title of the plot. In this example, the title is set to “Bar Chart Example”. You can customize the title by changing the value assigned to title_text to better describe the content or purpose of your bar chart.\n\n\n\nTitle and subtitle - Title w/ Subtitle 1\n\n\nShow the code\nfig = px.bar(data, x='cty', y='hwy')\n\nfig.update_layout(\n    title_text=\"City vs Highway MPG Bar Chart\",\n    title_font=dict(color=\"red\"),\n    title_font_size=18,\n    title_y=0.95,\n    title_x=0.5\n)\n\nfig.add_annotation(\n    text=\"Your Annotation Text\",\n    font=dict(color=\"blue\"),  # Choose your desired font color\n    x=0.5,\n    y=0.9,\n    showarrow=False\n)\nfig.show()\n\n\n                                                \n\n\n\n\n\n\n\n\nTitle and subtitle - Title w/ Subtitle 1 Explanation\n\n\n\n\n\nThe title_text parameter in the update_layout method is used to set the main title of the plot, and additional parameters like title_font, title_font_size, title_y, and title_x are used to customize the appearance and position of the title.\nThe add_annotation method is used to add an annotation or subtitle to the plot. In this example, it adds a blue text annotation with the content “Your Annotation Text” at a specified position (x=0.5, y=0.9) relative to the plot.\nAdjust the values of these parameters to customize the appearance and position of the title and annotation according to your preferences.\n\n\n\nTitle and subtitle - Title w/ Subtitle 2\n\n\nShow the code\nfig = px.bar(data, x='cty', y='hwy')\n\nfig.update_layout(\n    title_text=\"City vs Highway MPG Bar Chart\",\n    title_font_size=18,\n    title_y=0.95,\n    title_x=0.5\n)\n\nfig.add_annotation(\n    text=\"Your Annotation Text\",\n    x=0.5,\n    y=0.9,\n    showarrow=False\n)\n\nfig.show()\n\n\n                                                \n\n\n\n\n\n\n\n\nTitle and subtitle - Title w/ Subtitle 2 Explanation\n\n\n\n\n\nThe title_text parameter in the update_layout method is used to set the main title of the plot, and additional parameters like title_font_size, title_y, and title_x are used to customize the appearance and position of the title.\nThe add_annotation method is used to add an annotation or subtitle to the plot. In this example, it adds a text annotation with the content “Your Annotation Text” at a specified position (x=0.5, y=0.9) relative to the plot.\nAdjust the values of these parameters to customize the appearance and position of the title and annotation according to your preferences.\n\n\n\nGroup Variable\n\n\nShow the code\nfig = px.bar(data, x='manufacturer', y=['cty', 'hwy'], barmode='group')\n\nfig.update_layout(\n    xaxis_title=\"Manufacturer\",\n    yaxis_title=\"Mileage\",\n    title_text=\"Average City and Highway Mileage by Manufacturer\"\n)\n\nfig.show()\n\n\n                                                \n\n\n\n\n\n\n\n\nGroup Variable Explanation\n\n\n\n\n\nThe x parameter in the px.bar function is set to ‘manufacturer’, which means the bars will be grouped by the ‘manufacturer’ variable on the x-axis. The y parameter is set to a list [‘cty’, ‘hwy’], indicating that two sets of bars will be plotted for each manufacturer, one for ‘cty’ and another for ‘hwy’.\nThe barmode='group' parameter ensures that the bars are grouped for each ‘manufacturer’.\nThe update_layout method is used to set the titles for the x-axis (xaxis_title), y-axis (yaxis_title), and the overall plot (title_text). In this example, the plot represents the average city and highway mileage by manufacturer.\n\n\n\nAxis formatting - Axis Scale removing Zero\n\n\nShow the code\nfig = px.bar(data, x='manufacturer', y=['cty', 'hwy'], barmode='group')\n\nfig.update_layout(\n    xaxis_title=\"Manufacturer\",\n    yaxis_title=\"Mileage\",\n    title_text=\"Average City and Highway Mileage by Manufacturer\",\n    xaxis=dict(showline=True, showgrid=False),\n    yaxis=dict(zeroline=False, showline=True, showgrid=False),\n)\n\nfig.show()\n\n\n                                                \n\n\n\n\n\n\n\n\nAxis formatting - Axis Scale removing Zero Explanation\n\n\n\n\n\nThe xaxis_title and yaxis_title parameters in the update_layout method are used to set the titles for the x-axis and y-axis, respectively.\nThe xaxis and yaxis dictionaries in the update_layout method provide additional formatting options for the x-axis and y-axis. In this example:\nxaxis=dict(showline=True, showgrid=False) ensures that the x-axis has a visible line but no grid lines. yaxis=dict(zeroline=False, showline=True, showgrid=False) ensures that the y-axis has no zero line (zeroline=False), a visible line, and no grid lines. These settings help customize the appearance of the plot by controlling the visibility of axis lines and grid lines.\n\n\n\nAxis formatting - Axis Domain Sizing\n\n\nShow the code\nfig = px.bar(data, x='manufacturer', y=['cty', 'hwy'], barmode='group')\n\nfig.update_layout(\n    xaxis_title=\"Manufacturer\",\n    yaxis_title=\"Mileage\",\n    title_text=\"Average City and Highway Mileage by Manufacturer\",\n    xaxis=dict(domain=[0.1, 0.9]),  # Adjust the domain as needed\n)\n\nfig.show()\n\n\n                                                \n\n\n\n\n\n\n\n\nAxis formatting - Axis Domain Sizing Explanation\n\n\n\n\n\nThe xaxis_title and yaxis_title parameters in the update_layout method are used to set the titles for the x-axis and y-axis, respectively.\nThe xaxis dictionary in the update_layout method includes the domain parameter, which is set to [0.1, 0.9]. This parameter controls the size of the x-axis domain, determining the portion of the total width of the plot that the x-axis occupies. In this example, the x-axis is set to span from 10% to 90% of the total width.\nAdjust the values of the domain parameter as needed to control the sizing of the x-axis in your plot.\n\n\n\nReference marks - Verticle Reference Line with Color\n\n\nShow the code\nfig = px.bar(data, x='manufacturer', y=['cty', 'hwy'], barmode='group')\n\nfig.update_layout(\n    xaxis_title=\"Manufacturer\",\n    yaxis_title=\"Mileage\",\n    title_text=\"Average City and Highway Mileage by Manufacturer\"\n)\n\nfig.add_shape(\n    dict(\n        type=\"line\",\n        x0=\"Your_X_Value\",  # Specify the x-coordinate of the line\n        x1=\"Your_X_Value\",  # Specify the x-coordinate of the line\n        y0=0,\n        y1=1,\n        line=dict(color=\"red\"),  # Specify the color of the line\n    )\n)\n\nfig.show()\n\n\n                                                \n\n\n\n\n\n\n\n\nReference marks - Verticle Reference Line with Color Explanation\n\n\n\n\n\nThe add_shape method is used to add a reference line to the plot. In this example, a vertical reference line is added to the x-axis at the specified x-coordinate value.\nThe type=\"line\" parameter specifies that the added shape is a line. The x0 and x1 parameters are set to “Your_X_Value” to specify the x-coordinates where the line starts and ends. Replace “Your_X_Value” with the actual x-coordinate value where you want the reference line.\nThe y0 and y1 parameters set the starting and ending points on the y-axis. In this example, they are set to 0 and 1, respectively.\nThe line dictionary inside the shape specifies the attributes of the line, such as the color. In this case, the line color is set to red. Adjust the values as needed to customize the appearance of the reference line.\n\n\n\nReference marks - Verticle Reference Line with Text\n\n\nShow the code\nfig = px.bar(data, x='manufacturer', y=['cty', 'hwy'], barmode='group')\n\nfig.update_layout(\n    xaxis_title=\"Manufacturer\",\n    yaxis_title=\"Mileage\",\n    title_text=\"Average City and Highway Mileage by Manufacturer\"\n)\n\nfig.add_shape(\n    dict(\n        type=\"line\",\n        x0=\"Your_X_Value\",  # Specify the x-coordinate of the line\n        x1=\"Your_X_Value\",  # Specify the x-coordinate of the line\n        y0=0,\n        y1=1,\n        line=dict(color=\"red\"),  # Specify the color of the line\n    )\n)\n\nfig.add_annotation(\n    text=\"Your Text\",\n    x=\"Your_X_Value\",  # Specify the x-coordinate of the text\n    y=500,  # Adjust the y-coordinate of the text as needed\n    showarrow=False\n)\n\nfig.show()\n\n\n                                                \n\n\n\n\n\n\n\n\nReference marks - Verticle Reference Line with Text Explanation\n\n\n\n\n\nThe add_shape method is used to add a reference line to the plot. In this example, a vertical reference line is added to the x-axis at the specified x-coordinate value.\nThe type=\"line\" parameter specifies that the added shape is a line. The x0 and x1 parameters are set to “Your_X_Value” to specify the x-coordinates where the line starts and ends. Replace “Your_X_Value” with the actual x-coordinate value where you want the reference line.\nThe y0 and y1 parameters set the starting and ending points on the y-axis. In this example, they are set to 0 and 1, respectively.\nThe line dictionary inside the shape specifies the attributes of the line, such as the color. In this case, the line color is set to red.\nThe add_annotation method is used to add text annotation to the plot. The text parameter is set to “Your Text”, and the x and y parameters specify the coordinates where the text will be placed. Adjust the values of these parameters as needed to customize the appearance of the reference line and text.",
    "crumbs": [
      "Workbooks",
      "Project 2"
    ]
  },
  {
    "objectID": "Setup/quarto_setup.html",
    "href": "Setup/quarto_setup.html",
    "title": "Quarto for Data Science",
    "section": "",
    "text": "Quarto\n\nQuarto is an open-source scientific and technical publishing system built on Pandoc. You can create dynamic content with Python, R, Julia, and Observable.\nWe use this perfect union of Jupyter Notebooks and RMarkdown for reporting on our projects. It leverages Markdown and Python code chunks to create dynamic HTML content.\n\n\nMarkdown\nMarkdown is a plain text formatting syntax aimed at making writing more accessible. The philosophy behind Markdown is that plain text documents should be readable without tags making a mess, but there should still be ways to add text modifiers like lists, bold, italics, etc. It is an alternative to WYSIWYG (what you see is what you get) editors, which use rich text that later gets converted to proper HTML.\n\n\nQuarto Basics\nYou will need to install the Quarto CLI and then go through the VS Code directions on using Quarto with Python.\n\nInstall Quarto CLI \nDownload the class instructional template. Open it in VS Code and press the Preview button. It should produce a HTML file with a Plotly Express Chart and a data table. (If it errors, it may be missing some libraries. Here is the code to install them.)\n\n\n\nIf you still can not Preview your .qmd template file. Run quarto check in your Terminal section of VS Code and copy paste the ouput in a DM to your teacher or TA.\n\n\n\n\nQuarto Preview Tip\nWhen clicking on the Preview Icon  in the top right of your .qmd file, some students experience the preview rendering their entire course website. If this is the case, you can fix it by only opening the project.qmd file you are working on in VS-Code instead of opening the entire course folder.\n\nContinue to Git and GitHub\nInstall Git and GitHub\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Setup",
      "Quarto"
    ]
  },
  {
    "objectID": "Setup/aws_setup.html",
    "href": "Setup/aws_setup.html",
    "title": "AWS Virtual Machine Setup",
    "section": "",
    "text": "Prerequisites\nIf you want to use a AWS Academy VM for this course, you will need to have an AWS Educate account. Please email the admin for with the subject line “AWS Educate Account Request” and include your full name and course code. You will receive an email from AWS Educate with instructions on how to create an account.\n\n\nAWS Virtual Machine Setup\nWatch and follow the two videos, and or follow along with the AWS Virtual Machine Creation Document. Copy paste code from the cell below.\n\n\n\n\nSetup AWS Documentation\n\nAWS Virtual Machine Creation\n\n\n\nCopy Paste Code Block\ngit clone https://github.com/byui-bwh/db-workstation-automation.git \n\ncd db-workstation-automation/\n\n. ./provision_vm.sh\n\n\nClone this GitHub Repository\ncd Desktop\ngit clone https://github.com/NicholasBoss/clarkstudents24.git\n\n\nInstalling GitHub Desktop\n\nOpen the Repo just cloned in Visual Studio Code\nNavigate through code to week 2 and open the root.py file\nRun the code in the root.py file by pressing the play button in the top right corner of the file\n\nWhen asked to run the GitHub desktop script answer y\nWhen asked to run the file setup script answer n\nClose the file\n\n\n\n\nInstalling Quarto CLI\n\nNavigete to the Quarto CLI website for Linux\n\nRun all the termainal commands on the site in the terminal in the AWS instance\n\n\n\n\nInstalling the Quarto VS Code Extension\n\nOpen Visual Studio Code\nClick on the Extensions icon in the left side bar\nSearch for Quarto in the search bar\nClick the Install button on the Quarto extension\nClose the Extensions tab\n\n\n\nInstall Python Libraries\n\nOpen the terminal in Visual Studio Code\nRun the following command to install the required Python libraries\n\npip install numpy pandas scikit-learn plotly.express nbformat nbclient pyyaml jupyter\n\n\nInstall Other VS Code Extensions\n\nJupyter\nLive Share\nPython\nPylance\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Setup",
      "AWS VM (Optional)"
    ]
  },
  {
    "objectID": "Setup/python_lib.html",
    "href": "Setup/python_lib.html",
    "title": "Python for Data Science",
    "section": "",
    "text": "Install Python Libraries\n\n\n\nInstalling and Importing Packages\nThe Apple Silicon is still more difficult to get installed. You can use the following links to get it installed - Link 1, Link 2, Link 3.\nWe can get packages installed for this course using one of the two methods below.\n\nUsing your interactive Python (Jupyter server)\nThis is the preferred install method for both PC and Mac:\n#%%\n# copy paste this into a python file in vs code and run this cell\nimport sys\n!{sys.executable} -m pip install numpy pandas scikit-learn lets-plot palmerpenguins nbformat nbclient pyyaml\n\n\nUsing your terminal (alternative method)\n# default way\npip install numpy pandas scikit-learn lets-plot palmerpenguins nbformat nbclient pyyaml\n\n\n\n\nLearn More about the Packages you Installed\nWe want to install the following three packages;\n\npandas\nnumpy\nplotly express\nscikit-learn.\n\n\nLearn More About Python\nPython\n\n\nContinue to Install VS Code\nInstall VS Code\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Setup",
      "Python Libraries"
    ]
  },
  {
    "objectID": "Setup/vs_code_setup.html",
    "href": "Setup/vs_code_setup.html",
    "title": "VS Code for Data Science",
    "section": "",
    "text": "Download and Install Visual Studio Code\n\nUse the link below to download and install the latest version of VS Code:\n\nVS Code\n\n\n\nInstall VS Code Extensions\nUse these links to install the VS Code Extensions. Note: DO NOT install python when you install the python extension. You will install python from a different source in the next step.\n\nJupyter\nLive Share\nPython\nPylance\nQuarto\n\n\n\nLearn More About VS Code\nVS Code\n\n\nContinue to Install Quarto\nQuarto\n\n\n\n\n Back to top",
    "crumbs": [
      "Setup",
      "VS Code"
    ]
  },
  {
    "objectID": "Setup/python_setup.html",
    "href": "Setup/python_setup.html",
    "title": "Python for Data Science",
    "section": "",
    "text": "Download and Install Python\n\nWe need to download and install the latest version of python approved by your professor. (Python 3.11.7)\nNote: [do not] install python from VS Code or the Microsoft Store or Anaconda Python.org is the only place you should install VS Code from (link below)\n\nPython\n\n\nInstall Python on Windows\nMake sure to check the box that says Add Python to PATH before clicking Install Now. Sometimes it is phrased as Add Python to environment variables.\n\n\n\nContinue to Install Python Libraries\nInstall Python Libraries\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Setup",
      "Python"
    ]
  },
  {
    "objectID": "Projects/project_5.html",
    "href": "Projects/project_5.html",
    "title": "Project 5: The War with Star Wars",
    "section": "",
    "text": "Walkthrough\n\n\n\nBackground\nSurvey data is notoriously difficult to munge. Even when the data is recorded cleanly the options for ‘write in questions’, ‘choose from multiple answers’, ‘pick all that are right’, and ‘multiple choice questions’ makes storing the data in a tidy format difficult.\nIn 2014, FiveThirtyEight surveyed over 1000 people to write the article titled, America’s Favorite ‘Star Wars’ Movies (And Least Favorite Characters). They have provided the data on GitHub.\nFor this project, your client would like to use the Star Wars survey data to figure out if they can predict an interviewing job candidate’s current income based on a few responses about Star Wars movies.\n\n\nClient Request\nThe Client is who performed the survey but outsourced the analitics to a 3rd party. They want you to clean up the data so you can: a. Validate the data provided on GitHub lines up with the article by recreating 2 of the visuals from the article a. Determine if you predict if a person from the survey makes more than $50k\n\n\nData\nDownload: StarWars.csv\nInformation: Article\n\n\nReadings\n\nP4DS: CH6 Tidy Data (Skim)\nP4DS: CH14 Graphics for Communication (Skim)\nP4DS: CH16 Numbers (Read)\nP4DS: CH17 Strings and Text (Read)\nP4DS: Ch18 Regular Expressions (Read)\nP4DS: CH19 Categorical Data (Read)\n\n\nOptional References\n\nWhy to not use get_dummies\n\n\n\n\nQuestions and Tasks (Core)\n\nShorten the column names and clean them up for easier use with pandas. Provide a table or list that exemplifies how you fixed the names.\nClean and format the data so that it can be used in a machine learning model. As you format the data, you should complete each item listed below. In your final report provide example(s) of the reformatted data with a short description of the changes made.\n\nFilter the dataset to respondents that have seen at least one film\n\nCreate a new column that converts the age ranges to a single number. Drop the age range categorical column\n\nCreate a new column that converts the education groupings to a single number. Drop the school categorical column\n\nCreate a new column that converts the income ranges to a single number. Drop the income range categorical column\n\nCreate your target (also known as “y” or “label”) column based on the new income range column\n\nOne-hot encode all remaining categorical columns\n\nValidate that the data provided on GitHub lines up with the article by recreating 2 of the visuals from the article.\nBuild a machine learning model that predicts whether a person makes more than $50k. Describe your model and report the accuracy.\n\n\n\nQuestions and Tasks (Stretch)\nHere is an example Stretch question(s) for this project. Your instructor may assign different Stretch question(s). You must comment in Canvas when submitting your project if you completed any of the Stretch questions.\n\nBuild a machine learning model that predicts whether a person makes more than $50k. With accuracy of at least 65%. Describe your model and report the accuracy.\nValidate the data provided on GitHub lines up with the article by recreating a 3rd visual from the article.\nCreate a new colum that converts the location groupings to a single number. Drop the location categorical column.\n\n\n\nSubmission:\n\n\n\n\n\n\nNote\n\n\n\n\n\nWhen you have completed the report, you will need to follow this process to submit your work:\n\nBefore you begin you must Click the link in the email from GitHub to join your class group in the BYUI-math-dept org in GitHub. (If you have not received an email, please DM your teacher in Slack, this invitation does expire after 7 days so please dont delay in accepting the invitation)\nHave the Course Work Portfolio open in VS Code\nClick Termanal in VS Code and open a new terminal\nIn the terminal, type quarto render and press enter\n\nThis will render the entire course work portfolio into HTML files\nThis will move all those files into the docs folder\nThis can take a few minutes to complete\nIf there is an error in any cell of the quarto files, the rendering will stop and you will need to fix the error before rendering again (if you get stuck post your error in Slack)\n\nOnce the report is rendered, open the GitHub Desktop application\nType a summary of the changes in the Summary box\nClick Commit to main blue button in the bottom left corner\nClick Push origin blue button in the middle right of the screen\n\n\n\n\n\n\nDeliverables:\nUse this template to submit your Client Report. The template has two sections:\n\nA short elevator pitch that highlights key values or metrics from the results. Describing these key insights to interest or hook the reader to want to read more about your work. The writing style should be more technical with some creative elements. Do not summarize what you did.\n\nAnswers to the questions | tasks. Each should include a written description of your results, code cells with comments, charts and/or tables.\n\nA short summary of work must be submitted in the comments in Canvas wwhen you submit the URL. Rate your own work on a scale of 1-5. 1 being poor and 5 being excellent. Include a short description of why you rated your work the way you did.\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Projects",
      "Project 5: The War with Star Wars"
    ]
  },
  {
    "objectID": "Projects/project_2.html",
    "href": "Projects/project_2.html",
    "title": "Project 2: Late Flights & Missing Data (JSON)",
    "section": "",
    "text": "Walkthrough\n\n\n\nBackground\n\n\n\n\n\n\nNote\n\n\n\n\n\nWe will complete six projects during the semester that each take about two weeks (four days of class). On average, a student will spend 2 hours outside of class per hour in class to complete the assigned readings, submit any Canvas items, and complete the project (for a total of 8 hours per project). The instruction for each project will be structured into sections as written on this page.\nThis first Background section provides context for the project. Make sure you read the background carefully to see the big picture needs and purpose of the project.\n\n\n\nDelayed flights are not something most people look forward to. In the best case scenario you may only wait a few extra minutes for the plane to be cleaned. However, those few minutes can stretch into hours if a mechanical issue is discovered or a storm develops. Arriving hours late may result in you missing a connecting flight, job interview, or your best friend’s wedding.\nIn 2003 the Bureau of Transportation Statistics (BTS) began collecting data on the causes of delayed flights. The categories they use are Air Carrier, National Aviation System, Weather, Late-Arriving Aircraft, and Security. You can visit the BTS website to read definitions of these categories.\n\n\nClient Request\nThe JSON file for this project contains information on delays at 7 airports over 10 years. Your task is to clean the data, search for insights about flight delays, and communicate your results to the Client. The Client is a CEO of a flight booking app who is interested in the causes of flight delays and wants to know which airports have the worst delays. They also want to know the best month to fly if you want to avoid delays of any length.\n\n\nData\n\n\n\n\n\n\nNote\n\n\n\n\n\nEvery data science project should start with data, and our class projects are no different. Each project will have ‘Download’ and ‘Information’ links like the ones below.\n\n\n\nDownload: JSON File\nInformation: Data Description\nSubject Matter: Types of Delay\n\n\nReadings\n\n\n\n\n\n\nNote\n\n\n\n\n\nThe Readings section will contain links to reading assignments that are required for each project, as well as optional references. Remember that you are reading this material to build skills. Take the time to comprehend the readings and the skills contained within.\nWe recommend reading through the assigned material once for a general understanding before the first day of each project. You will reread and reference the material multiple times as you complete the project.\n\n\n\n\nP4DS: CH4 Data Transformation (Read)\nP4DS: CH6 Tidy Data (Read)\nP4DS: CH11 Visualization (Read)\nP4DS: CH12 Layers (Skim)\nP4DS: CH13 Exploratory Data Analysis (Skim)\nP4DS: CH21 Missing Values (Read)\nP4DS: Ch25.3 JSON (Read)\n\n\nOptional References\n\nPython Data Science Handbook: Missing Data\nHandling Missing Data\nWikipedia Missing Data\nisin method\nwhere method\nnp.where method\nreplace method\nAn introduction to JSON (May need to open in ingognito to read.)\nThe key word in ‘Data Science’ is not Data…\nHow to Handle Missing Data (May need to open in ingognito to read.)\nLambda Function\n\n\n\n\nQuestions and Tasks (Core)\n\n\n\n\n\n\nNote\n\n\n\n\n\nThis section lists the questions and tasks that need to be completed for the project. Your work on the project must be compiled into a report, pushed to GitHub and a URL submitted in Canvas by the weekend following the last day of material for the project.\nThere are two types of questions: Core and Stretch. Core questions are required for each project. The course syllabus competencies requires specic a number of projects having all the Stretch questions achived based on your goals for the grade level you are seeking.\n\n\n\n\nFix all of the varied missing data types in the data to be consistent (all missing values should be displayed as “NaN”). In your report include one record example (one row) from your new data, in the raw JSON format. Your example should display the “NaN” for at least one missing value.__\nWhich airport has the worst delays? Describe the metric you chose, and why you chose it to determine the “worst” airport. Your answer should include a summary table that lists (for each airport) the total number of flights, total number of delayed flights, proportion of delayed flights, and average delay time in hours.\nWhat is the best month to fly if you want to avoid delays of any length? Describe the metric you chose and why you chose it to calculate your answer. Include one chart to help support your answer, with the x-axis ordered by month. (To answer this question, you will need to remove any rows that are missing the Month variable.)\nAccording to the BTS website, the “Weather” category only accounts for severe weather delays. Mild weather delays are not counted in the “Weather” category, but are actually included in both the “NAS” and “Late-Arriving Aircraft” categories. Your job is to create a new column that calculates the total number of flights delayed by weather (both severe and mild). You will need to replace all the missing values in the Late Aircraft variable with the mean. Show your work by printing the first 5 rows of data in a table. Use these three rules for your calculations:\n\n100% of delayed flights in the Weather category are due to weather\n\n30% of all delayed flights in the Late-Arriving category are due to weather\n\nFrom April to August, 40% of delayed flights in the NAS category are due to weather. The rest of the months, the proportion rises to 65%\n\nUsing the new weather variable calculated above, create a barplot showing the proportion of all flights that are delayed by weather at each airport. Describe what you learn from this graph.\n\n\n\nQuestions and Tasks (Stretch)\nHere is an example Stretch question(s) for this project. Your instructor may assign different Stretch question(s). You must comment in Canvas when submitting your project if you completed any of the Stretch questions.\n\nWhich delay is the worst delay? Create a similar analysis as above for Weahter Delay with: Carrier Delay and Security Delay. Compare the proportion of delay for each of the three categories in a Chart and a Table. Describe your results.\n\n\n\nSubmission:\n\n\n\n\n\n\nNote\n\n\n\n\n\nWhen you have completed the report, you will need to follow this process to submit your work:\n\nBefore you begin you must Click the link in the email from GitHub to join your class group in the BYUI-math-dept org in GitHub. (If you have not received an email, please DM your teacher in Slack, this invitation does expire after 7 days so please dont delay in accepting the invitation)\nHave the Course Work Portfolio open in VS Code\nClick Termanal in VS Code and open a new terminal\nIn the terminal, type quarto render and press enter\n\nThis will render the entire course work portfolio into HTML files\nThis will move all those files into the docs folder\nThis can take a few minutes to complete\nIf there is an error in any cell of the quarto files, the rendering will stop and you will need to fix the error before rendering again (if you get stuck post your error in Slack)\n\nOnce the report is rendered, open the GitHub Desktop application\nType a summary of the changes in the Summary box\nClick Commit to main blue button in the bottom left corner\nClick Push origin blue button in the middle right of the screen\n\n\n\n\n\n\nDeliverables:\n\n\n\n\n\n\nNote\n\n\n\n\n\nDeliverables are “the quantifiable goods or services that must be provided upon the completion of a project”. In this class the deliverable for each project is a GitHub published report created using Quarto files. This final section will be the same for each project.\n\n\n\nUse this template to submit your Client Report. The template has two sections:\n\nA short elevator pitch that highlights key values or metrics from the results. Describing these key insights to interest or hook the reader to want to read more about your work. The writing style should be more technical with some creative elements. Do not summarize what you did.\n\nAnswers to the questions | tasks. Each should include a written description of your results, code cells with comments, charts and/or tables.\n\nA short summary of work must be submitted in the comments in Canvas wwhen you submit the URL. Rate your own work on a scale of 1-5. 1 being poor and 5 being excellent. Include a short description of why you rated your work the way you did.\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nYour report should be written in quarto markdown files and pushed to GitHub. Submit a URL of the rendered project in Canvas. (Do not submit the URL to the GitHub .qmd file)\n\n\n\n\n\nFeedback:\n\n\n\n\n\n\nNote\n\n\n\n\n\nYou will recieve feedback and/or coaching notes in the form of a GitHub issue. You will need to address the feedback, re-render and resubmit the project, and mark the GitHub issue as closed.\n\n\n\n\n\nResubmission:\n\n\n\n\n\n\nNote\n\n\n\n\n\nYou will have one opportunity to resubmit the project after you have received feedback. The window for the resubmission will be open through the Wednesday following the due date of the project. Therefore it is recomended that you turn in a draft of the project early on the Thursday before the due date to ensure you have time to address any feedback and resubmit the project. It is acceptable to turn in a draft that is only 80% complete. This will allow you to get feedback on the majority of the project and then focus on the final details. The closer to that Thursday you turn in the draft the more feedback and coaching you will recieve.\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Projects",
      "Project 2: Late Flights and Missing Data (JSON)"
    ]
  },
  {
    "objectID": "Projects/project_0.html",
    "href": "Projects/project_0.html",
    "title": "Project 0: Introduction",
    "section": "",
    "text": "Walkthrough\n\n\n\nBackground\n\n\n\n\n\n\nNote\n\n\n\n\n\nWe will complete six projects during the semester that each take about two weeks (four days of class). On average, a student will spend 2 hours outside of class per hour in class to complete the assigned readings, submit any Canvas items, and complete the project (for a total of 8 hours per project). The instruction for each project will be structured into sections as written on this page.\nThis first Background section provides context for the project. Make sure you read the background carefully to see the big picture needs and purpose of the project.\n\n\n\nPython and VS Code are tools commonly used in the field of data science. During our first two days of class we will get VS Code prepped for data science programming. Completing Project 0 will set you up for success the rest of the semester.\n\n\nData\n\n\n\n\n\n\nNote\n\n\n\n\n\nEvery data science project should start with data, and our class projects are no different. Each project will have ‘Download’ and ‘Information’ links like the ones below.\n\n\n\nDownload: penguins data\n\n\nReadings\n\n\n\n\n\n\nNote\n\n\n\n\n\nThe Readings section will contain links to reading assignments that are required for each project, as well as optional references. Remember that you are reading this material to build skills. Take the time to comprehend the readings and the skills contained within.\nWe recommend reading through the assigned material once for a general understanding before the first day of each project. You will reread and reference the material multiple times as you complete the project.\n\n\n\nThe readings listed below are required for the first two days of class.\n\nCourse Setup (Do)\nLearn about our Book Python for Data Science (Skim)\nPY4DS: First Steps (Skim)\nPY4DS: CH1 Whole Game (Skim)\nPY4DS: CH2 Data Visualization (Read)\nP4DS: CH30 Markdown (Read)\nP4DS: CH31 Quarto (Skim)\nQuarto Instructional Template for DS (Read)\n\n\nOptional References\n\nLearn about VS Code\nVS Code user interface\nReading Technical Documentation\n\n\n\n\nQuestions and Tasks\n\n\n\n\n\n\nNote\n\n\n\n\n\nThis section lists the questions and tasks that need to be completed for the project. Your work on the project must be compiled into a report, pushed to GitHub and a URL submitted in Canvas by the weekend following the last day of material for the project.\n\n\n\nIn the DS 250 folder of the Course Work Portfolio, edit the Project0.qmd quarto file to build a report that includes the following:\n\nFinish the Course Setup, and post any questions to getting your environment working smoothly in your course Slack channel\n\nRecreate the example chart from PY4DS: CH2 Data Visualization of the textbook.\nInclude the table created from PY4DS: CH2 Data Visualization used to create the above chart\n\nUpdate the _quarto.yml file in course work portfolio:\n\nupdate the title to includ your name\nupdate the site url to your published site url (this will be in the deployment section in GitHub)\nupdate the repo-url to your GitHub repository url\nupdate the issue-url to your GitHub issue url (its the same as the repo url but with /issues/new at the end)\nupdate the page-footer left to include your name\nupdate the page-footer right href to include your LinkedIn url (optional)\n\nUpdate the resume.qmd file in course work portfolio to include your updated resume using markdown. See P4DS: CH30 Markdown\n\n\n\nSubmission:\n\n\n\n\n\n\nNote\n\n\n\n\n\nWhen you have completed the report, you will need to follow this process to submit your work:\n\nBefore you begin you must Click the link in the email from GitHub to join your class group in the BYUI-math-dept org in GitHub. (If you have not received an email, please DM your teacher in Slack, this invitation does expire after 7 days so please dont delay in accepting the invitation)\nHave the Course Work Portfolio open in VS Code\nClick Termanal in VS Code and open a new terminal\nIn the terminal, type quarto render and press enter\n\nThis will render the entire course work portfolio into HTML files\nThis will move all those files into the docs folder\nThis can take a few minutes to complete\nIf there is an error in any cell of the quarto files, the rendering will stop and you will need to fix the error before rendering again (if you get stuck post your error in Slack)\n\nOnce the report is rendered, open the GitHub Desktop application\nType a summary of the changes in the Summary box\nClick Commit to main blue button in the bottom left corner\nClick Push origin blue button in the middle right of the screen\n\n\n\n\n\n\nDeliverables:\n\n\n\n\n\n\nNote\n\n\n\n\n\nDeliverables are “the quantifiable goods or services that must be provided upon the completion of a project”. In this class the deliverable for each project is a GitHub published report created using Quarto files. This final section will be the same for each project.\n\n\n\nUse this template to submit your Client Report. The template has two sections:\n\nA short elevator pitch that highlights key values or metrics from the results. Describing these key insights to interest or hook the reader to want to read more about your work. The writing style should be more technical with some creative elements. Do not summarize what you did.\n\nAnswers to the questions | tasks. Each should include a written description of your results, code cells with comments, charts and/or tables.\n\nA short summary of work must be submitted in the comments in Canvas wwhen you submit the URL. Rate your own work on a scale of 1-5. 1 being poor and 5 being excellent. Include a short description of why you rated your work the way you did.\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nYour report should be written in quarto markdown files and pushed to GitHub. Submit a URL of the rendered project in Canvas. (Do not submit the URL to the GitHub .qmd file)\n\n\n\n\n\nFeedback:\n\n\n\n\n\n\nNote\n\n\n\n\n\nYou will recieve feedback and/or coaching notes in the form of a GitHub issue. You will need to address the feedback, re-render and resubmit the project, and mark the GitHub issue as closed.\n\n\n\n\n\nResubmission:\n\n\n\n\n\n\nNote\n\n\n\n\n\nYou will have one opportunity to resubmit the project after you have received feedback. The window for the resubmission will be open through the Wednesday following the due date of the project. Therefore it is recomended that you turn in a draft of the project early on the Thursday before the due date to ensure you have time to address any feedback and resubmit the project. It is acceptable to turn in a draft that is only 80% complete. This will allow you to get feedback on the majority of the project and then focus on the final details. The closer to that Thursday you turn in the draft the more feedback and coaching you will recieve.\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Projects",
      "Project 0: Introduction"
    ]
  },
  {
    "objectID": "Workbooks/wb1.html",
    "href": "Workbooks/wb1.html",
    "title": "Project 1 Workbook",
    "section": "",
    "text": "The data science lab is a resource you can use in person, online, and in Slack.",
    "crumbs": [
      "Workbooks",
      "Project 1"
    ]
  },
  {
    "objectID": "Workbooks/wb1.html#tutoring-lab-info",
    "href": "Workbooks/wb1.html#tutoring-lab-info",
    "title": "Project 1 Workbook",
    "section": "",
    "text": "The data science lab is a resource you can use in person, online, and in Slack.",
    "crumbs": [
      "Workbooks",
      "Project 1"
    ]
  },
  {
    "objectID": "Workbooks/wb1.html#text-basics",
    "href": "Workbooks/wb1.html#text-basics",
    "title": "Project 1 Workbook",
    "section": "Text Basics",
    "text": "Text Basics\n\nHorizontal Lines\nAdd horizontal lines with either three ---, ***, or ___ But you also need blank lines above and below them\n\n\n\n\n\n\nExpand To See The Results\n\n\n\n\n\n\n\n\n\n\n\n\nHeaders\n# Level 1 Header\n## Level 2 Header\n### Level 3 Header\n#### Level 4 Header\n##### Level 5 Header\n###### Level 6 Header\nNote: only top 3 Levels of Headers will automatically generate a table of contents. Also Level 2 will automatically add a line underneath it.\n\n\n\n\n\n\nExpand To See The Results\n\n\n\n\n\nLevel 1 Header\n\nLevel 2 Header\n\nLevel 3 Header\n\nLevel 4 Header\n\nLevel 5 Header\n\nLevel 6 Header\n\n\n\n\n\n\n\n\n\n\nItalics and Bold\n_italics_ use one `_`\nyou can also use _mid_ sentence\n\n__bold__ use two `__`\nyou can also use __mid__ sentence\n\n\n\n\n\n\nExpand To See The Results\n\n\n\n\n\nitalics use one _ you can also use mid sentence\nbold use two __ you can also use mid sentence\n\n\n\n\n\nBullet Items\n- Bulleted items\n  - Indented bulleted items\n  - You can have as many as you want\n    - Really as many as you want\n      - I knew you wanted one more\n\n\n\n\n\n\nExpand To See The Results\n\n\n\n\n\n\nBulleted items\n\nIndented bulleted items\nYou can have as many as you want\n\nReally as many as you want\n\nI knew you wanted one more bullet\n\n\n\n\n\n\n\n\n\nNumbered Items\n1. Numbered items\n1. Numbered items continued\n1. Dont worry these will iterate\n1. Keep using 1. each time\n\n\n\n\n\n\nExpand To See The Results\n\n\n\n\n\n\nNumbered items\nNumbered items continued\nDont worry these will iterate\nKeep using 1. each time",
    "crumbs": [
      "Workbooks",
      "Project 1"
    ]
  },
  {
    "objectID": "Workbooks/wb1.html#level-2-header",
    "href": "Workbooks/wb1.html#level-2-header",
    "title": "Project 1 Workbook",
    "section": "Level 2 Header",
    "text": "Level 2 Header\n\nLevel 3 Header\n\nLevel 4 Header\n\nLevel 5 Header\n\nLevel 6 Header",
    "crumbs": [
      "Workbooks",
      "Project 1"
    ]
  },
  {
    "objectID": "Workbooks/wb1.html#pandas-dataframe-df",
    "href": "Workbooks/wb1.html#pandas-dataframe-df",
    "title": "Project 1 Workbook",
    "section": "Pandas DataFrame (df)",
    "text": "Pandas DataFrame (df)\n\n\n\n\n\n\nExpand To See Links to Chapter in the book Python4DS\n\n\n\n\n\n\nChapter on Pandas: DataFrames\n\n\n\n\nWhat is a pandas dataFrame? We can read the official documentation. I also like the video in this tutorial.\nUse the Import Packages and Load df for the Code that follows.",
    "crumbs": [
      "Workbooks",
      "Project 1"
    ]
  },
  {
    "objectID": "Workbooks/wb1.html#import-packages",
    "href": "Workbooks/wb1.html#import-packages",
    "title": "Project 1 Workbook",
    "section": "Import Packages",
    "text": "Import Packages\nimport `library` as `alias`\n\n\nImport Libraries\n\n#| label: libraries\n#| include: false\nimport pandas as pd\nimport numpy as np\nimport plotly.express as px\n\nfrom IPython.display import Markdown\nfrom IPython.display import display",
    "crumbs": [
      "Workbooks",
      "Project 1"
    ]
  },
  {
    "objectID": "Workbooks/wb1.html#load-data",
    "href": "Workbooks/wb1.html#load-data",
    "title": "Project 1 Workbook",
    "section": "Load Data",
    "text": "Load Data\ndf = pd.read_csv(`url` or `file_path`)\n\n\nLoad Data\n\n#| label: project data\n#| code-summary: Read and format project data\n# Include and execute your code here\nurl = \"https://github.com/byuidatascience/data4names/raw/master/data-raw/names_year/names_year.csv\"\ndf = pd.read_csv(url)\n\nData Frames come with attributes and built-in functions that can help us get a feel for our df.\nRun the code below one at a time (or use other functions of your choice) to explore the names df. What do you learn?\n\n\n.columns\n\ndf.columns\n\n\n\n.shape\n\ndf.shape\n\n\n\n.size\n\ndf.size\n\n\n\n.head()\n\ndf.head()\n\n\n\n.describe()\n\ndf.describe()",
    "crumbs": [
      "Workbooks",
      "Project 1"
    ]
  },
  {
    "objectID": "Workbooks/wb1.html#pandas-data-transformation",
    "href": "Workbooks/wb1.html#pandas-data-transformation",
    "title": "Project 1 Workbook",
    "section": "Pandas Data Transformation",
    "text": "Pandas Data Transformation\n\n\n\n\n\n\nExpand To See Links to Chapter in the book Python4DS\n\n\n\n\n\n\nChapters Transformation: 15-22",
    "crumbs": [
      "Workbooks",
      "Project 1"
    ]
  },
  {
    "objectID": "Course Materials/vs_code.html",
    "href": "Course Materials/vs_code.html",
    "title": "VS Code for Data Science",
    "section": "",
    "text": "Learn How to Install VS Code\nVS Code\n\n\n\nWhat if my interactive Python window in VS Code is not using the same version of Python as my terminal?\nYou can set your Python version in VS Code by opening a .py script and then clicking on the Python text in the bottom left corner as shown below.\n\nOnce you click, VS Code will open the command pallete where you can select your installation of Python that you would like to use with this workspace.\n\nThis setting will not fix what version your interactive Python window is using. You can get there by opening settings by using the ⌘, shortcut.\nYou can then search your settings for jupyter and you should see a section that has Jupyter Command Line Arguments. Click on the Edit in settings.json.\n\nHere you can set the jupyter path to Python to match the one you picked for your Terminal. An example for a Mac computer is shown below.\n\"python.pythonPath\": \"/usr/local/opt/python/bin/python3\",\n\n\n\nWhat if I am not able to read in files from the GitHub links using read_csv()?\nMost likely your Python SSl certificates are not installed. Follow the answer in this post.\n\n\n\nHow do I use VS Code to collaborate?”\nMicrosft’s Live Share extension documentation says, ‘Live Share enables you to quickly collaborate with a friend, classmate, or professor on the same code without the need to sync code or to configure the same development tools, settings, or environment.’ You can follow their guide or use our course created video.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Materials",
      "VS Code"
    ]
  },
  {
    "objectID": "Skill Builders/git_github.html#before-you-start",
    "href": "Skill Builders/git_github.html#before-you-start",
    "title": "GitHub and Git",
    "section": "Before you start",
    "text": "Before you start\nMake sure you have gone through the tutorial on under course materials called Git: we assume that you have a connection to your data.\n\nComplete the Hello World GitHub Guide",
    "crumbs": [
      "Skill Builders",
      "Project 6: GitHub and Git"
    ]
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "About this site\n\n\n\n Back to top"
  },
  {
    "objectID": "skill_builders.html",
    "href": "skill_builders.html",
    "title": "Skill Builders",
    "section": "",
    "text": "These short activites are provided for you to gain some additional skills to help with the class projects.\n\n\n\n Back to top",
    "crumbs": [
      "Skill Builders"
    ]
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "We will be relying on a few resources for this course. You will find the pertinant readings attached to each of the projects. Those readings will be culled from:\n\nPython for Data Science: A port of R for Data Science using the Python packages pandas and Altair.\npandas User Guide\nPlotly-Express User Guide\nscikit-learn learn User Guide\nscikit-learn Tutorials\nPython Data Science Handbook\nA Whirlwind Tour of Python\nSQL\n\nWes McKinney’s pandas code for his book Python for Data Analysis is a useful reference as well: https://github.com/wesm/pydata-book\n\n\n\n Back to top",
    "crumbs": [
      "Projects"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DS 250: Data Science Programming",
    "section": "",
    "text": "Use the top menue to navigate to the main sections of the course\nOnce in a main section switch to navigating using the left side bar\n\n\n\nThis section contains all the needed info to get your computer setup for the course\n\n\n\nProjects contains all the course projects for the semester\n\n\n\nAdditional information on specific topics can be found here and usesd as a resourse. This can expand upon the initial setup for the course and build on that\n\n\n\nThese are under development. A workbook is designed to mirror the code in the reading from each project with links back to the book. If you learn from a more hands on approach starting with the workbook coudl be a good approach\n\n\n\nYou can find your professor, see their calendar, and book time to meet one-on-one\n\n\n\nThe syllabus and course competencies are located here. Take time to review them as they contain all the info to help you get the grade you want out of the course\n\n\n\nProfessors may use these as group coding activities in class\n\n\n\nLook here for common questions on the course"
  },
  {
    "objectID": "index.html#how-to-navigate-this-course",
    "href": "index.html#how-to-navigate-this-course",
    "title": "DS 250: Data Science Programming",
    "section": "",
    "text": "Use the top menue to navigate to the main sections of the course\nOnce in a main section switch to navigating using the left side bar\n\n\n\nThis section contains all the needed info to get your computer setup for the course\n\n\n\nProjects contains all the course projects for the semester\n\n\n\nAdditional information on specific topics can be found here and usesd as a resourse. This can expand upon the initial setup for the course and build on that\n\n\n\nThese are under development. A workbook is designed to mirror the code in the reading from each project with links back to the book. If you learn from a more hands on approach starting with the workbook coudl be a good approach\n\n\n\nYou can find your professor, see their calendar, and book time to meet one-on-one\n\n\n\nThe syllabus and course competencies are located here. Take time to review them as they contain all the info to help you get the grade you want out of the course\n\n\n\nProfessors may use these as group coding activities in class\n\n\n\nLook here for common questions on the course"
  },
  {
    "objectID": "Workbooks/wb6.html",
    "href": "Workbooks/wb6.html",
    "title": "Project 6 Workbook",
    "section": "",
    "text": "Project 6 WorkBook\nUnder Construction\n\n\n\n\n Back to top",
    "crumbs": [
      "Workbooks",
      "Project 6"
    ]
  },
  {
    "objectID": "Workbooks/wb3.html",
    "href": "Workbooks/wb3.html",
    "title": "Project 3 Workbook",
    "section": "",
    "text": "The data science lab is a resource you can use in person, online, and in Slack.",
    "crumbs": [
      "Workbooks",
      "Project 3"
    ]
  },
  {
    "objectID": "Workbooks/wb3.html#tutoring-lab-info",
    "href": "Workbooks/wb3.html#tutoring-lab-info",
    "title": "Project 3 Workbook",
    "section": "",
    "text": "The data science lab is a resource you can use in person, online, and in Slack.",
    "crumbs": [
      "Workbooks",
      "Project 3"
    ]
  },
  {
    "objectID": "Skill Builders/relational_data.html",
    "href": "Skill Builders/relational_data.html",
    "title": "SQL & Databases",
    "section": "",
    "text": "For this skill builder, we are exploring some important topics in relational databases. This exercise will require you to create SQL queries through python. You may want to at least scan the readings before beginning this task since this serves as an assessment of your understanding of the assigned readings.\nThis should be able to be finished within 75 minutes. Work through it on your own or in a group based in your professors instruction.",
    "crumbs": [
      "Skill Builders",
      "Project 3: SQL & Databases"
    ]
  },
  {
    "objectID": "Skill Builders/relational_data.html#skill-builder",
    "href": "Skill Builders/relational_data.html#skill-builder",
    "title": "SQL & Databases",
    "section": "",
    "text": "For this skill builder, we are exploring some important topics in relational databases. This exercise will require you to create SQL queries through python. You may want to at least scan the readings before beginning this task since this serves as an assessment of your understanding of the assigned readings.\nThis should be able to be finished within 75 minutes. Work through it on your own or in a group based in your professors instruction.",
    "crumbs": [
      "Skill Builders",
      "Project 3: SQL & Databases"
    ]
  },
  {
    "objectID": "Skill Builders/relational_data.html#before-you-start",
    "href": "Skill Builders/relational_data.html#before-you-start",
    "title": "SQL & Databases",
    "section": "Before you start",
    "text": "Before you start\nMake sure you have installed VS-code, pandas, and Plotly Express on your computer.\nAlso make sure you have gone through the tutorial on under course materials called SQL for Data Science: we assume that you have a connection to your data.\n\n\nExercise 1\n\nReadme file\nA database can consist of more than one table/data set. A relational database consists of tables/data sets that share columns. These shared columns then establish the relationship between the tables, thus the name relational database. The relations are sometimes not easily found and they require careful investigations.\nTo understand what is in a relational database, we can start with understanding the tables and the columns within.\nHere is a link to the readme file of the baseball database.\n\nWhat is the name of the table that records data about pitchers in the regular seasons?\n\n\nWhat do the HR and HBP columns mean in that table respectively?\n\n\n\n\n\nExcercise 2\n\nSELECT and FROM\nThe simplest SQL query is a query with SELECT and FROM. These are the keywords you will see again and again in SQL. Usually, when constructing a more complex query, it is easier to identify what goes into these two clauses first.\n\nCreate a query that shows all columns from the table you found in Exercise 1, save the dataframe in a variable “pitch”\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nresult = pd.read_sql_query(\n    'SELECT _______ FROM _______',\n    con)\n\nresults\n\n\n\n\n\n\n\nExcercise 3\n\nWHERE\nThe WHERE keyword allows us to filter down the table horizontally (fewer rows).\nIt goes after SELECT and FROM.\n\nUsing a SQL query, select all rows in the same table where HR is lesser than 10 and gs is greater than 25.\n\n\nFind out what the columns mean and explain your query in words\n\n\n\n\n\nExcercise 4\n\nORDER BY\nORDER BY sort the table you select by one or more columns and goes after WHERE\n\nUsing the same query in exercise 2, edit it so that the table is ordered by the year of the season (nearest to furthermost) and the player ID (alphabetically).\n\n\n\n\n\nExcercise 5\n\nJoins\nJoins are used when you wish to create a new table through two different tables. Keep in mind that you have to identify the relationship between two tables before you can correctly join them.\nJOIN goes between FROM and WHERE.\n\nIdentify the shared columns (keys) and join the table in exercise 2 with the salaries table, then filter the data so that it shows only pitchers in the year 1986.\n\nYou should get a dataframe with 306 rows.\n\n\n\n\nExercise 6\n\nGroup by\nGroup by is a keyword we use to lower the level of granularity of a table. Meaning we are combining rows into one by the given column(s).\nCreate a query that captures the number of pitchers the Washington Nationals used in each year, then sort the table by year\nYou should get a dataframe with 23 rows.",
    "crumbs": [
      "Skill Builders",
      "Project 3: SQL & Databases"
    ]
  },
  {
    "objectID": "Skill Builders/relational_data.html#because-youre-extra",
    "href": "Skill Builders/relational_data.html#because-youre-extra",
    "title": "SQL & Databases",
    "section": "Because You’re Extra",
    "text": "Because You’re Extra\n\nExercise 7\nResearch the order of operations for SQL and put the following keywords in that order.\n\nSELECT\nFROM\nJOIN\nWHERE\nHAVING\nORDER BY\nGROUP BY\nLIMIT\n\n\n\n\n\n\n\n\nAfter you have completed this skill builder with your team (or on your own) then compare your work to our script\n\n\n\n\n\nSee the script.",
    "crumbs": [
      "Skill Builders",
      "Project 3: SQL & Databases"
    ]
  },
  {
    "objectID": "Skill Builders/pandas_altair.html",
    "href": "Skill Builders/pandas_altair.html",
    "title": "Pandas and Altair",
    "section": "",
    "text": "For this skill builder, we are exploring some important functions in the package of pandas and Altair. DS programming requires a lot of data wrangling. Using the proper functions, we can create concise and comprehensive codes. You should be exposed to a few functions through the readings this week.\nYou may want to at least scan the readings before beginning this task since this serves as an assessment of your understanding of the assigned readings. This should be able to be finished within 60 minutes. You should work through it on your own or in a group based in your professors instruction."
  },
  {
    "objectID": "Skill Builders/pandas_altair.html#skill-builder",
    "href": "Skill Builders/pandas_altair.html#skill-builder",
    "title": "Pandas and Altair",
    "section": "",
    "text": "For this skill builder, we are exploring some important functions in the package of pandas and Altair. DS programming requires a lot of data wrangling. Using the proper functions, we can create concise and comprehensive codes. You should be exposed to a few functions through the readings this week.\nYou may want to at least scan the readings before beginning this task since this serves as an assessment of your understanding of the assigned readings. This should be able to be finished within 60 minutes. You should work through it on your own or in a group based in your professors instruction."
  },
  {
    "objectID": "Skill Builders/pandas_altair.html#data-import",
    "href": "Skill Builders/pandas_altair.html#data-import",
    "title": "Pandas and Altair",
    "section": "Data Import",
    "text": "Data Import\nRun the following code to import the data we need for this skill builder:\n\n# package import\nimport numpy as np\nimport pandas as pd\nimport altair as alt\n\n# data import\nurl = 'https://raw.githubusercontent.com/vincentarelbundock/Rdatasets/master/csv/AER/Guns.csv'\ndf = pd.read_csv(url)\nMake sure the variable df is correctly assigned in your environment and finish the following exercises. You can read the documentation of the data on this page - https://vincentarelbundock.github.io/Rdatasets/doc/AER/Guns.html\n\n\nExercise 1\nOne of the first things we can do to a freshly imported data is to check its columns. This will help us understand the basic structure of the dataframe(table).\n\nUsing one line of code, select all the columns in dat, assign it to a variable called col_list.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nEvery dataframe has an attribute “columns”.\nAccessing this attribute will give you a list of all column names\n\n\n\nWe often want to know the dimension of a dataframe. How many columns are in the dataset? How many rows are in the dataset?\n\nUsing one line of code, show the number of columns and rows in df.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nEvery dataframe has an attribute “shape”.\nAccessing this attribute will give you the dimension of a datafarme\n\n\n\nNow run df.head(). It will print out the first 5 rows of data in df.\n\nJust from looking at the output, what column(s) seems to be redundant with the row number?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nThere is one column that serves as nothing but a row counter, that columns is redundant.\n\n\n\n\n\n\nExercise 2\nAfter a brief investigation of the data, we will clean up the data. By cleaning up, we are trying to filter down df so this only holds data we need. We will first get rid of the extra column we found in the previous excercise.\n\nUsing one line of code, drop the redundant column using the variable col_list (created in excercise 1)\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse drop().\nUnderstand what “axis” is as a parameter of drop().\nYour function should looks like this:\ndf.drop([col_list[_]], axis = _)\nfill the “_“’s with the correct values and assign the output to df.\n\n\n\nDon’t forget to save the changes in df. Run df.head() to make sure the column is dropped in df.\n\n\n\nExercise 3\nWe have filtered df vertically by dropping a column. Now we will try to filter df horizontally, meaning we will get rid of some the rows.\nWe can do that by applying a condition to df. A condition is an expression that can be evaluated as True/False. For example, 8 &gt; 5 is an expression that evaluates to be True. This is trivial because 8 will always be greater than 5.\nRun the code below:\n\nwhat is the difference between exp1 and exp2?\n\nexp1 = 8 &gt; 5\nexp2 = df.violent &lt; 300\n\n\n\n\n\n\nHint\n\n\n\n\n\nTry type() on else variable OR calling else variable.\n\n\n\nRun ths code below:\n\nBy putting df.violent &lt; 300, and the violent column from df into a dataframe, what is the relationship between the two columns?\n\nexp = pd.DataFrame({\"df.violent &lt; 300\" : exp2,\n                    \"violent value from dat\" : df.violent})\n\nexp\n\n\n\n\n\n\nHint\n\n\n\n\n\nTry computing df.violent[n] &lt; 300 and (df.violent &lt; 300)[n] where n is less than the number of row. The two expressions will always be the same as long as n is less than the number of rows.\n\n\n\n\nUsing query()to filter down the df so that it only contains the data for idaho\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nquery() takes in expressions and filters down data.\n\n\n\nDon’t forget to save the changes in df. Run df.shape() to make sure the there are 23 rows and 13 columns.\n\n\n\nExercise 4\nBesides filtering, we can manipulate the data by adding new data to it. By adding a new column to the data, we assign a new value to each row.\n\nUsing assign(), create a new column that show the ratio between murder rate and violent rate.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse assign()\nYou see get the ratio by computing this code:\ndf.murder/df.violent\n\n\n\n\n\n\nExercise 5\n\nCreate a scatter plot that shows the relationship between murder rate and violent rate for the state of Idaho. Your chart should show murder rate as the x-axis, violent as the y-axis.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nCan you mimic this plot?\nhttps://altair-viz.github.io/gallery/scatter_tooltips.html"
  },
  {
    "objectID": "Skill Builders/pandas_altair.html#because-youre-extra",
    "href": "Skill Builders/pandas_altair.html#because-youre-extra",
    "title": "Pandas and Altair",
    "section": "Because You’re Extra",
    "text": "Because You’re Extra\n\nExercise 6\n\nUsing a line of code, filter down the data set so that it only shows the data in years between 1993 and 1997.\n\n\n\n\nExercise 7\n\nCreate a line chart that show prisoners numbers for the state of Idaho, Utah, and Oregon.\n\nYour chart should show year as the x-axis, prisoner as the y-axis, states as different colours, along with an appropriate title.\n\n\n\nExercise 8\n\nWithout using query(), finshed the data wrangling in question 2,5 and 6.\n\n\n\n\n\n\n\n\nAfter you have completed this skill builder with your team (or on your own) then compare your work to our script\n\n\n\n\n\nSee the script."
  },
  {
    "objectID": "Skill Builders/ml_sklearn.html#data",
    "href": "Skill Builders/ml_sklearn.html#data",
    "title": "Machine Learning",
    "section": "Data",
    "text": "Data\nLink to data",
    "crumbs": [
      "Skill Builders",
      "Project 4: Machine Learning"
    ]
  },
  {
    "objectID": "Skill Builders/ml_sklearn.html#intro-to-titanic-machine-learning-skill-builder",
    "href": "Skill Builders/ml_sklearn.html#intro-to-titanic-machine-learning-skill-builder",
    "title": "Machine Learning",
    "section": "Intro to Titanic Machine Learning Skill Builder",
    "text": "Intro to Titanic Machine Learning Skill Builder\nFor this skill builder, we’ll be putting our machine learning hats on. We’ll be creating a model that predicts whether a passenger survived. With machine learning, there is a lot of jargon! It can be quite overwhelming at times. This skill builder attempts to keep things basic and simple. With that being said, there are some terms that are important to understand. Let’s look at the first few rows of our dataset before proceeding with the definitions.\nThe titanic dataset will be used for examples of each definition.\n\n\n\n\n\n\n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsiblings_spouses_aboard\nparents_children_aboard\nfare\n\n\n\n\n0\n3\n1\n22\n1\n0\n7.25\n\n\n1\n1\n0\n38\n1\n0\n71.2833\n\n\n1\n3\n0\n26\n0\n0\n7.925\n\n\n1\n1\n0\n35\n1\n0\n53.1\n\n\n0\n3\n1\n35\n0\n0\n8.05\n\n\n\n\nImportant Terms:\n\nfeatures: measurable property of the object you’re trying to predict. We use this information to predict our target of interest.\n\nExample: pclass, sex, age, siblings_spouses_aboard , parents_children_aboard, fare columns are all examples of different features.\nSynonyms: attributes, explanatory variables, independent variables, variables, X’s, covariates\n\ntarget: the feature that you are wanting to gain more insight into. The thing you are trying to predict.\n\nExample: in the titanic dataset our target is survived\nSynonyms: label, dependent variable, y\n\ntrain set: Usually 70% of the rows from the original dataset are randomly sampled to create this training data. It’s used by the algorithm, to determine, or learn, the optimal combinations of variables that will generate a good predictive model\n\nExample: Random sample of 70% of the original titanic dataset rows\nSynonyms: training data, train data, X_train, y_train\n\ntest set: Usually the remaining 30% of the rows in the original dataset are used to create this dataset. The testing data is a set of rows used only to assess the performance (i.e. generalization) of a model. To do this, the final model is used to predict classifications of examples in the test set. Those predictions are compared to the examples’ true classifications to assess the model’s accuracy.\n\nExample: Random sample of 30% of the original titanic dataset rows\nSynonyms: testing data, test data, X_test, y_test\n\nevaluation metrics: A statistic that tells you how well your predictions align with the actual values. Other words, tells you how good your model is.\n\nExample: Accuracy, Precision, Recall, MSE, MAE, Rsquared\nSynonyms: performance metric\n\n\nAgain, this is a very light and oversimplified treatment of machine learning. The purpose of this project is to help you understand the main concepts of ml and walk you through the process of building a machine learning model. A simplified work flow of a machine learning project is shown below. Spend some time getting familiar with this flow &mdash as you are about to code it… Exciting!\nNote in order to do this skill builder you will need to have scikit-learn installed on your machine. Run the following command in your terminal if you haven’t already.\npip install scikit-learn\n\n\n\nExercise 1\n\nImports and Loading in Data\n# Loading in packages\nimport pandas as pd\nimport numpy as np\nimport plotly.express as px\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Loading in data\ndata = pd.read_csv(___)\n\n\n\n\nExercise 2\nCreate a chart exploring the relationship between age and survived in the titanic dataset. A strip plot, density plot, or boxplot might be useful here. Below is an example of a density plot. Feel free to replicate this chart or create your own.\nThe purpose of making this chart is to explore the relationships between a feature and the target. We want to see if the feature contains predictive information about the target. This is a large part of machine learning called Exploratory Data Analysis that should never be skipped! Spend time getting to know your features and how they interact with other features and the target.\n\n\n\n\nExercise 3\nBuild a random forest model that is able to predict whether a passenger survived. This exercise is the bulk of the skill builder and contains several steps.\n\nStep 0: Split the data into X and y variables\nThe X variable will contain all your features\n# Removes the target and keeps all features\nX = data.drop(___, axis=1)  \nThe y variable will hold the target\n# Selects the target column\ny = data['___']  \n\n\nStep 1: Split data into train and test sets\nThe train_test_split function is useful for this task. Review the train_test_split function documentation\n# Splitting X and y variables into train and test sets using stratified sampling\nX_train, X_test, y_train, y_test = train_test_split(___, ___, test_size=0.3,\n                                                    random_state=24, stratify=y)\n\n\nStep 2: Train the model\nExplore the RandomForestClassifier documentation for the RandomForestClassifier. It’s not necessary to understand the inner workings of the Random Forest algorithm for this class - just learn the syntax of fitting the model.\n# Creating random forest object\nrf = RandomForestClassifier(random_state=24)  \n\n# Fit with the training data\nrf.fit(___, ___)  \n\n\nStep 3: Use test set to make predictions\n# Using the features in the test set to make predictions\ny_pred = rf.predict(___)  \n\n\nStep 4: Compare test set predictions to actual values. Calculate the accuracy.\n# Comparing predictions to actual values\naccuracy_score(___, ___)  \n\n\n\n\nExercise 4\nWhat is the most important feature in making predictions? Why do you think this is?\nCreate a table that shows the feature importances in descending order. The random forest classifier has a feature importances attribute. It can be accessed by rf.feature_importances_. The table should look something like this.\n\n\n\nfeature names\nimportances\n\n\n\n\nfare\n0.288051\n\n\nsex\n0.281853\n\n\nage\n0.266491\n\n\npclass\n0.0814224\n\n\nsiblings_spouses_aboard\n0.0475633\n\n\nparents_children_aboard\n0.034619\n\n\n\n\n\n\n\n\n\n\nAfter you have completed this skill builder with your team (or on your own) then compare your work to our script\n\n\n\n\n\nSee the script.",
    "crumbs": [
      "Skill Builders",
      "Project 4: Machine Learning"
    ]
  },
  {
    "objectID": "Skill Builders/introduction.html",
    "href": "Skill Builders/introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "This should be able to be finished within 60 minutes. You should work through it on your own or in a group based in your professors instruction. This serves as an assessment of your understanding of the assigned readings.",
    "crumbs": [
      "Skill Builders",
      "Project 0: Introduction"
    ]
  },
  {
    "objectID": "Skill Builders/introduction.html#skill-builder",
    "href": "Skill Builders/introduction.html#skill-builder",
    "title": "Introduction",
    "section": "",
    "text": "This should be able to be finished within 60 minutes. You should work through it on your own or in a group based in your professors instruction. This serves as an assessment of your understanding of the assigned readings.",
    "crumbs": [
      "Skill Builders",
      "Project 0: Introduction"
    ]
  },
  {
    "objectID": "Skill Builders/introduction.html#before-you-start",
    "href": "Skill Builders/introduction.html#before-you-start",
    "title": "Introduction",
    "section": "Before You Start",
    "text": "Before You Start\nMake sure you have installed VS-code, pandas, and plotly express on your computer. You can install these package by typing this line in the terminal.\npip install pandas plotly.express\nOR if you have more than one version of python\npip3.10 install pandas plotly.express\npip3.10 indicates the version of python you are installing the packages to.\n\n\nGet Familiar With Your Tools\nProgramming involves a lot of research. Unlike subjects like Mathematics or History, we are not required to remember every single function and its usage. It is natural for experienced programmers to look for answers on the internet, books, even from other people’s code. Programming will be extremely frustrating if we are not allowed to do web searches, so please get familiar with the tools you have and use them often.\n\n\n\nOffical Documentation\nThis should be your first resort for understanding any code/function. Scanning the documentation of a function will allow you to get an overview of its usage.\nHere is a link to the documentation of the assign() function:\nhttps://pandas.pydata.org/docs/reference/api/pandas.DataFrame.assign.html\nExample of assign() (as shown in the documentation)\n    import pandas as pd\n\n    df1 = pd.DataFrame({'temp_c': [17.0, 25.0]},\n                  index=['Portland', 'Berkeley'])\n    \n    df2 = df1.assign(temp_f=df1.temp_c * 9 / 5 + 32)\n\n\n\nExercise 1\nAfter reading the documentation for assign(), write a short paragraph to explain assign() as if you were talking to someone with zero programming experience (use the example above to help you explain assign()).\n\nWhat is the difference between df1 and df2?\nHow was df2 derived from df1?\n\n\nOnline Textbook\nIt pains us to see students would rather be stuck at problems for hours yet they refuse to use the textbook. This is another very useful resource since this is designed for this class. link to the textbook: https://byuidatascience.github.io/python4ds/\n\n\n\n\nExercise 2\nLocate the section where the textbook talks about query() and answer these questions.\n\nWhat function in R’s dplyr is equivalent or comparable to query() in pandas (You should include the section number in your answer)?\nWhat is the easiest mistake for python beginner to make that was shown in the text about query() (You should include the section number in your answer)?\n\n\nThe Internet\nGoogle is a programmer’s friend. Get used to googling thing, in fact, you want to be an expert in googling\n\nQuestion that cannot be answered by the textbook and documentation? Google it.\n\nA function you have never seen before? Google it.\nAn error in your code? Google it.\n\n\n\n\n\nExercise 3\nProvide at least 2 extra resources you could find about the pandas function drop() on the internet.\n\nTutor, TA (Through Slack, Zoom, or In-Person)\nWe want to help you with your work; we want to answer your questions; but most importantly, we want to help you succeed in this class. That will require you to put in the necessary time in understanding the readings, coding and debugging. When you ask us a question, we expect that you have read the documentation, searched the textbook, and done your own research. Then we can be most helpful and can provide insights on top of your understanding.\n\n\nExamples of Bad Questions\n\nHow does drop() work? We will ask you to read the documentation for drop().\nHow do you make a table in a markdown file? We will refer you to the textbook.\nI don’t want these columns in my data, how can I drop them? We will ask you if you have found any things on the internet.\n\n\n\nExamples of good questions\n\nI am still confused about the syntax of drop(). After reading the documentation, this is my understanding of the function… . What am I missing?\nI tried making a table in markdown (show code), it is still not giving me what I want, how can I fix this?\nI am trying to drop these columns in my dataframe, I think drop() is what I am looking for. Am I in the right direction? If not, what keywords should I be googling?\n\n\n\n\n\nExercise 4\nUsing the code and tools mentioned above, finish question 4 and 5 under 3.2.4 in the textbook.(use the data in mpg for your plot):\n# library import\nimport pandas as pd \nimport plotly.express as px\n\n# data import\nurl = \"https://github.com/byuidatascience/data4python4ds/raw/master/data-raw/mpg/mpg.csv\"\n\nmpg = pd.read_csv(url)\n\nQuestion 4: Make a scatterplot of hwy vs cyl.\nQuestion 5: What happens if you make a scatterplot of class vs drv? Why is the plot not useful?\n\n\n\n\n\n\n\n\nAfter you have completed this skill builder with your team (or on your own) then compare your work to our script\n\n\n\n\n\nSee the script.",
    "crumbs": [
      "Skill Builders",
      "Project 0: Introduction"
    ]
  },
  {
    "objectID": "Setup/copilot_setup.html",
    "href": "Setup/copilot_setup.html",
    "title": "Getting started with GitHub Copilot",
    "section": "",
    "text": "GitHub Copilot is free to use for verified students, teachers, and maintainers of popular open source projects.",
    "crumbs": [
      "Setup",
      "Copilot"
    ]
  },
  {
    "objectID": "Setup/copilot_setup.html#footnotes",
    "href": "Setup/copilot_setup.html#footnotes",
    "title": "Getting started with GitHub Copilot",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n1↩︎\n2↩︎\nhttps://docs.github.com/en/copilot/using-github-copilot/getting-started-with-github-copilot#next-steps↩︎",
    "crumbs": [
      "Setup",
      "Copilot"
    ]
  },
  {
    "objectID": "Course Materials/python.html",
    "href": "Course Materials/python.html",
    "title": "Python for Data Science",
    "section": "",
    "text": "Python for Data Science python4DS is a port of R for Data Science (2e) into Python. The authors keep Garrett Grolemund and Hadley Wickham’s writing and examples as much as possible while demonstrating Python instead of R. The book focuses on pandas and LetsPlot in the Python code snippets.\nThis book will teach you how to do data science with Python: You’ll learn how to get your data into Python, get it into the most useful structure, transform it, visualise it and model it. In this book, you will find a practicum of skills for data science. Just as a chemist learns how to clean test tubes and stock a lab, you’ll learn how to clean data and draw plots—and many other things besides. These are the skills that allow data science to happen, and here you will find the best practices for doing each of these things with Python. You’ll learn how to use the grammar of graphics, literate programming, and reproducible research to save time. You’ll also learn how to manage cognitive resources to facilitate discoveries when wrangling, visualising, and exploring data.\n\nLearn How to Setup Python\nPython\n\n\n\n\n Back to top",
    "crumbs": [
      "Materials",
      "Python for Data Science"
    ]
  },
  {
    "objectID": "Course Materials/git_pull_merge.html",
    "href": "Course Materials/git_pull_merge.html",
    "title": "Pull and Merge Forks on GitHub",
    "section": "",
    "text": "Create Pull Request\n\n\nGo the the forked repository in byuids-resumes and click Pull request.\n\n\n\n\n\nThis will bring you to the the following page where you need to click switching the base.\n\n\n\n\n\nNow you can Create pull request.\n\n\n\n\n\nHere you can type a note and then actually Create pull request.\n\n\n\n\n\nNow you need to View pull request.\n\n\n\n\n\nMerge Request\nIf you have admin access of the forked repository where you are doing the pull request, you can finish the next two steps.\n\n\nClick the Merge pull request button.\n\n\n\n\n\nNow confirm the merge.\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Materials",
      "GitHub - Pull and Merge Forks"
    ]
  },
  {
    "objectID": "Course Materials/ml.html",
    "href": "Course Materials/ml.html",
    "title": "Machine Learning",
    "section": "",
    "text": "Everyone seems to have a slightly different take on the differences between Artificial Intelligence, Machine Learning, and Data Science. The following four articles cover some of the most common definitions.\nAs you read them, think about the differences and similarities of the definitions. Given the backgrounds of the various authors, whose opinions might you give more weight to?\n\nMichael Copeland writing for NVidia\nBernard Marr writing for Forbes\nVincent Granville writing for Data Science Central\nSimply Statistics Blog - The key word in “Data Science” is not Data, it is Science\n\nOf particular note is this quote from the Granville article:\n\nEarlier in my career (circa 1990) I worked on image remote sensing technology, among other things to identify patterns (or shapes or features, for instance lakes) in satellite images and to perform image segmentation: at that time my research was labeled as computational statistics, but the people doing the exact same thing in the computer science department next door in my home university, called their research artificial intelligence. Today, it would be called data science or artificial intelligence, the sub-domains being signal processing, computer vision or IoT.\n\nAs with most things in the realm of science, there tends to be a wide gap between how the media, government, and business sectors view a particular technology compared to how it’s viewed by the engineers and scientists using that technology.\nFor our purposes in this course, we’ll define these terms as follows:\n\nArtificial Intelligence: The study of man-made “agents” that perceive their environment and take actions that maximize their chances of success at some goal.1\nMachine Learning: A subfield within Artificial Intelligence that gives “computers the ability to learn without being explicitly programmed.”2\nData Science: The study and use of the techniques, statistics, algorithms, and tools needed to extract knowledge and insights from data.3",
    "crumbs": [
      "Materials",
      "Machine Learning (ML)"
    ]
  },
  {
    "objectID": "Course Materials/ml.html#footnotes",
    "href": "Course Materials/ml.html#footnotes",
    "title": "Machine Learning",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nArtificial Intelligence: A Modern Approach by Russell and Norvig (Prentice Hall, 2009).↩︎↩︎\nSome Studies in Machine Learning Using the Game of Checkers, by Arthur L. Samuel (IBM Journal, Vol 3, No 3, 1959).↩︎↩︎\nWikipedia article on Data Science.↩︎↩︎\nMind Children, by Hans Moravec (Harvard University Press, 1988).↩︎↩︎\nXKCD 1425: Tasks.↩︎↩︎\nCambridge Alumni Magazine, Issue 79, pg 19.↩︎↩︎\nCambridge Alumni Magazine, Issue 79, pg 19.↩︎↩︎\nCross Validated: Prediction vs Inference.↩︎↩︎\nCross Validated: Prediction vs Inference.↩︎↩︎",
    "crumbs": [
      "Materials",
      "Machine Learning (ML)"
    ]
  },
  {
    "objectID": "Course Materials/sql.html",
    "href": "Course Materials/sql.html",
    "title": "SQL for Data Science",
    "section": "",
    "text": "There are many flavors of SQL but most flavors have the same base commands. SQL queries are typed in the following pattern;\nSELECT -- &lt;columns&gt; and &lt;column calculations&gt;\nFROM -- &lt;table name&gt;\n  JOIN -- &lt;table name&gt;\n  ON -- &lt;columns to join&gt;\nWHERE -- &lt;filter condition on rows&gt;\nGROUP BY -- &lt;subsets for column calculations&gt;\nHAVING -- &lt;filter conditions on groups&gt;\nORDER BY -- &lt;how the output is returned in sequence&gt;\nLIMIT -- &lt;number of rows to return&gt;\n\nIntroductory SQL links\n\nSQL Guide\nSELECT and FROM clauses\nWHERE and comparison operators\nORDER BY\nJoins\nAggregations\nGROUP BY\n\n\n\nLearn How to Setup SQLite\nSQLite\n\n\n\n\n Back to top",
    "crumbs": [
      "Materials",
      "SQLite"
    ]
  },
  {
    "objectID": "Setup/sql_setup.html",
    "href": "Setup/sql_setup.html",
    "title": "SQL for Data Science",
    "section": "",
    "text": "SQLITE Setup\nThere is nothing to download to setup SQLITE This SQLite Viewer VS Code extension will be helpful to explore the database\n\n\nDownloads\nDownload this sqlite db file Save it in the same place as the .py or .qmd file created in the next step\n\n\nTest Your Setup\nCopy the code below and test it in a .py file. If everything works you are all set\nimport pandas as pd \nimport numpy as np\nimport sqlite3\n\n# %%\n# careful to list your path to the file or save it in the same place as your .qmd or .py file\nsqlite_file = 'lahmansbaseballdb.sqlite'\ncon = sqlite3.connect(sqlite_file)\n\nq = 'SELECT * FROM allstarfull LIMIT 5'\nresults = pd.read_sql_query(q,con)\n\nresults\nYou can see the list of tables available in the database\nq = '''\n    SELECT * \n    FROM sqlite_master \n    WHERE type='table'\n    '''\ntable = pd.read_sql_query(q,con)\ntable.filter(['name'])\n\n\n\n\n Back to top",
    "crumbs": [
      "Setup",
      "SQL"
    ]
  },
  {
    "objectID": "Skill Builders/json_missing.html",
    "href": "Skill Builders/json_missing.html",
    "title": "JSONs & Missing",
    "section": "",
    "text": "Link to json file\n\n\n\n\nRead in the json file as a pandas dataframe. After reading in the data, you’ll want to explore it and gain some intuition. Exploring data is a very important step — the more you know about your data the better! Answer the following questions to gain some insight into this dataset.\n\nHow many rows are there?\nHow many columns?\nWhat does a row represent in this dataset?\nWhat are the different ways missing values are encoded?\nHow many np.nan in each column?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\n# Object/Categorical Columns\ndf.column_name.value_counts(dropna=False)\ndf.column_name.unique()\n\n# Numeric Columns\ndf.column_name.describe()\n\n# Counting missing values\ndf.isna().sum()  # Creates boolean dataframe and sums each column\n\n\n\n\n\n\n\nAfter learning different ways our data encodes missing values, now we will neatly manage them. There are many techniques we can use to handle missing values; for example, we can drop all rows that contain a missing value, impute with mean or median, or replace missing values with a new missing category. We will use some of these techniques in this exercise.\n\nshape_reported - replace missing values with missing string.\ndistance_reported - change -999 values to np.nan. (-999 is a typical way of encoding missing values.)\ndistance_reported - fill in missing values with the mean (imputation)\nwere_you_abducted - replace - string with missing string.\n\nThe first 10 rows of your data should look like this after completion of the above steps.\n\n\n\n\n\n\n\n\n\n\n\n\ncity\nshape_reported\ndistance_reported\nwere_you_abducted\nestimated_size\n\n\n\n\n0\nIthaca\nTRIANGLE\n8521.9\nyes\n5033.9\n\n\n1\nWillingboro\nOTHER\n7438.64\nno\n5781.03\n\n\n2\nHolyoke\nOVAL\n7438.64\nno\n697203\n\n\n3\nAbilene\nDISK\n7438.64\nno\n5384.61\n\n\n4\nNew York Worlds Fair\nLIGHT\n6615.78\nmissing\n3417.58\n\n\n5\nValley City\nDISK\n7438.64\nno\n4280.1\n\n\n6\nCrater Lake\nCIRCLE\n7377.89\nno\n528289\n\n\n7\nAlma\nDISK\n7438.64\nmissing\n4772.75\n\n\n8\nEklutna\nCIGAR\n5214.95\nno\n4534.03\n\n\n9\nHubbard\nCYLINDER\n8220.34\nmissing\n4653.72\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\ndf.column_name.replace(..., ..., inplace=True)\ndf.column_name.fillna(..., inplace=True)\n\n\n\n\n\n\n\nCreate a table that contains the following summary statistics. - median estimated size by shape - mean distance reported by shape - count of reports belonging to each shape\nYour table should look like this:\n\n\n\n\n\n\n\n\n\nshape_reported\nmedian_est_size\nmean_distance_reported\ngroup_count\n\n\n\n\nCIGAR\n5899.68\n6520.21\n3\n\n\nCIRCLE\n266002\n7408.26\n2\n\n\nCYLINDER\n4550.58\n8039.49\n2\n\n\nDISK\n4581.8\n7516.39\n16\n\n\nFIREBALL\n5407.22\n7097.78\n3\n\n\nFLASH\n6108.34\n7438.64\n1\n\n\nFORMATION\n5104.4\n8708.32\n2\n\n\nLIGHT\n3850.25\n7636.09\n2\n\n\nOTHER\n4699.4\n7473.98\n4\n\n\nOVAL\n4943.63\n7787.24\n4\n\n\nRECTANGLE\n3668.1\n6054.62\n2\n\n\nSPHERE\n5076.78\n7206.55\n6\n\n\nTRIANGLE\n5033.9\n8521.9\n1\n\n\nmissing\n250153\n7438.64\n2\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\n(df.groupby(...)\n     .agg(...,\n          ...,\n          ...))\n\n\n\n\n\n\n\nThe cities listed below reported their estimated size in square inches, not square feet. Create a new column named estimated_size_sqft in the dataframe, that has all the estimated sizes reported as sqft. (Hint: divide by 144 to go from sqin -&gt; sqft)\n\nHolyoke\nCrater Lake\nLos Angeles\nSan Diego\nDallas\n\nThe head of your data should look like this.\n\n\n\n\n\n\n\n\n\n\n\n\n\ncity\nshape_reported\ndistance_reported\nwere_you_abducted\nestimated_size\nestimated_size_sqft\n\n\n\n\n0\nIthaca\nTRIANGLE\n8521.9\nyes\n5033.9\n5033.9\n\n\n1\nWillingboro\nOTHER\n7438.64\nno\n5781.03\n5781.03\n\n\n2\nHolyoke\nOVAL\n7438.64\nno\n697203\n4841.69\n\n\n3\nAbilene\nDISK\n7438.64\nno\n5384.61\n5384.61\n\n\n4\nNew York Worlds Fair\nLIGHT\n6615.78\nmissing\n3417.58\n3417.58\n\n\n5\nValley City\nDISK\n7438.64\nno\n4280.1\n4280.1\n\n\n6\nCrater Lake\nCIRCLE\n7377.89\nno\n528289\n3668.68\n\n\n7\nAlma\nDISK\n7438.64\nmissing\n4772.75\n4772.75\n\n\n8\nEklutna\nCIGAR\n5214.95\nno\n4534.03\n4534.03\n\n\n9\nHubbard\nCYLINDER\n8220.34\nmissing\n4653.72\n4653.72\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nnp.where(...,  # Condition\n         ...,  # If condition is true\n         ...)  # If condition is false\n\n\n\n\n\n\n\n\n\n\nAfter you have completed this skill builder with your team (or on your own) then compare your work to our script\n\n\n\n\n\nSee the script.",
    "crumbs": [
      "Skill Builders",
      "Project 2: JSON & Missing"
    ]
  },
  {
    "objectID": "Skill Builders/json_missing.html#skill-builder",
    "href": "Skill Builders/json_missing.html#skill-builder",
    "title": "JSONs & Missing",
    "section": "",
    "text": "Link to json file\n\n\n\n\nRead in the json file as a pandas dataframe. After reading in the data, you’ll want to explore it and gain some intuition. Exploring data is a very important step — the more you know about your data the better! Answer the following questions to gain some insight into this dataset.\n\nHow many rows are there?\nHow many columns?\nWhat does a row represent in this dataset?\nWhat are the different ways missing values are encoded?\nHow many np.nan in each column?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\n# Object/Categorical Columns\ndf.column_name.value_counts(dropna=False)\ndf.column_name.unique()\n\n# Numeric Columns\ndf.column_name.describe()\n\n# Counting missing values\ndf.isna().sum()  # Creates boolean dataframe and sums each column\n\n\n\n\n\n\n\nAfter learning different ways our data encodes missing values, now we will neatly manage them. There are many techniques we can use to handle missing values; for example, we can drop all rows that contain a missing value, impute with mean or median, or replace missing values with a new missing category. We will use some of these techniques in this exercise.\n\nshape_reported - replace missing values with missing string.\ndistance_reported - change -999 values to np.nan. (-999 is a typical way of encoding missing values.)\ndistance_reported - fill in missing values with the mean (imputation)\nwere_you_abducted - replace - string with missing string.\n\nThe first 10 rows of your data should look like this after completion of the above steps.\n\n\n\n\n\n\n\n\n\n\n\n\ncity\nshape_reported\ndistance_reported\nwere_you_abducted\nestimated_size\n\n\n\n\n0\nIthaca\nTRIANGLE\n8521.9\nyes\n5033.9\n\n\n1\nWillingboro\nOTHER\n7438.64\nno\n5781.03\n\n\n2\nHolyoke\nOVAL\n7438.64\nno\n697203\n\n\n3\nAbilene\nDISK\n7438.64\nno\n5384.61\n\n\n4\nNew York Worlds Fair\nLIGHT\n6615.78\nmissing\n3417.58\n\n\n5\nValley City\nDISK\n7438.64\nno\n4280.1\n\n\n6\nCrater Lake\nCIRCLE\n7377.89\nno\n528289\n\n\n7\nAlma\nDISK\n7438.64\nmissing\n4772.75\n\n\n8\nEklutna\nCIGAR\n5214.95\nno\n4534.03\n\n\n9\nHubbard\nCYLINDER\n8220.34\nmissing\n4653.72\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\ndf.column_name.replace(..., ..., inplace=True)\ndf.column_name.fillna(..., inplace=True)\n\n\n\n\n\n\n\nCreate a table that contains the following summary statistics. - median estimated size by shape - mean distance reported by shape - count of reports belonging to each shape\nYour table should look like this:\n\n\n\n\n\n\n\n\n\nshape_reported\nmedian_est_size\nmean_distance_reported\ngroup_count\n\n\n\n\nCIGAR\n5899.68\n6520.21\n3\n\n\nCIRCLE\n266002\n7408.26\n2\n\n\nCYLINDER\n4550.58\n8039.49\n2\n\n\nDISK\n4581.8\n7516.39\n16\n\n\nFIREBALL\n5407.22\n7097.78\n3\n\n\nFLASH\n6108.34\n7438.64\n1\n\n\nFORMATION\n5104.4\n8708.32\n2\n\n\nLIGHT\n3850.25\n7636.09\n2\n\n\nOTHER\n4699.4\n7473.98\n4\n\n\nOVAL\n4943.63\n7787.24\n4\n\n\nRECTANGLE\n3668.1\n6054.62\n2\n\n\nSPHERE\n5076.78\n7206.55\n6\n\n\nTRIANGLE\n5033.9\n8521.9\n1\n\n\nmissing\n250153\n7438.64\n2\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\n(df.groupby(...)\n     .agg(...,\n          ...,\n          ...))\n\n\n\n\n\n\n\nThe cities listed below reported their estimated size in square inches, not square feet. Create a new column named estimated_size_sqft in the dataframe, that has all the estimated sizes reported as sqft. (Hint: divide by 144 to go from sqin -&gt; sqft)\n\nHolyoke\nCrater Lake\nLos Angeles\nSan Diego\nDallas\n\nThe head of your data should look like this.\n\n\n\n\n\n\n\n\n\n\n\n\n\ncity\nshape_reported\ndistance_reported\nwere_you_abducted\nestimated_size\nestimated_size_sqft\n\n\n\n\n0\nIthaca\nTRIANGLE\n8521.9\nyes\n5033.9\n5033.9\n\n\n1\nWillingboro\nOTHER\n7438.64\nno\n5781.03\n5781.03\n\n\n2\nHolyoke\nOVAL\n7438.64\nno\n697203\n4841.69\n\n\n3\nAbilene\nDISK\n7438.64\nno\n5384.61\n5384.61\n\n\n4\nNew York Worlds Fair\nLIGHT\n6615.78\nmissing\n3417.58\n3417.58\n\n\n5\nValley City\nDISK\n7438.64\nno\n4280.1\n4280.1\n\n\n6\nCrater Lake\nCIRCLE\n7377.89\nno\n528289\n3668.68\n\n\n7\nAlma\nDISK\n7438.64\nmissing\n4772.75\n4772.75\n\n\n8\nEklutna\nCIGAR\n5214.95\nno\n4534.03\n4534.03\n\n\n9\nHubbard\nCYLINDER\n8220.34\nmissing\n4653.72\n4653.72\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nnp.where(...,  # Condition\n         ...,  # If condition is true\n         ...)  # If condition is false\n\n\n\n\n\n\n\n\n\n\nAfter you have completed this skill builder with your team (or on your own) then compare your work to our script\n\n\n\n\n\nSee the script.",
    "crumbs": [
      "Skill Builders",
      "Project 2: JSON & Missing"
    ]
  },
  {
    "objectID": "Skill Builders/munging.html#data",
    "href": "Skill Builders/munging.html#data",
    "title": "Munging Data",
    "section": "Data",
    "text": "Data\nLink to the data",
    "crumbs": [
      "Skill Builders",
      "Project 5: Munging Data"
    ]
  },
  {
    "objectID": "Skill Builders/munging.html#intro-to-cleaning-movies-data",
    "href": "Skill Builders/munging.html#intro-to-cleaning-movies-data",
    "title": "Munging Data",
    "section": "Intro to cleaning movies data",
    "text": "Intro to cleaning movies data\nThis skill builder focuses on munging (formatting) data into a machine learning ready dataset. We will be using an IMDB Ratings dataset. It contains columns that are categorical. Sklearn cannot handle columns that are strings, so we need to convert these into a numerical representation. We accomplish this by either one hot encoding, label encoding, or taking just one value of the range provided. There are many other ways to represent these columns as numbers, but they are beyond the scope of this course.\nOnce you’ve converted all columns to numeric, in an intelligent way, you will be asked to recreate a graph using plotly express. Here is the head of the data you will be working with. Enjoy!\n\n\n\n\n\n\n\n\n\n\n\nstar_rating\ncontent_rating\ngenre\nduration\nbox_office_rev\nmajor_hit\n\n\n\n\n9.3\nR\nCrime\n142\n€1924521976 - €1925521976\nno\n\n\n9.2\nR\nCrime\n175\n€177034987 - €178034987\nno\n\n\n9.1\nR\nCrime\n200\n€2617541398 - €2618541398\nno\n\n\n9\nPG-13\nAction\n152\n€996115723 - €997115723\nno\n\n\n8.9\nR\nCrime\n154\n€1172054364 - €1173054364\nno\n\n\n\n\n\nExercise 1\n\nGrab the high range value for each movie and put it into a new column called high_range_rev.\n\nMake sure the data type of this new column is numeric!!\n\nRemove the box_office_rev column from the dataset.\n\nThe .str.split() and .astype() methods might be of use! Also, to get the euro sign just copy it from here, €, and put it in your code.\nThe first 5 rows of the resulting dataframe should look like this\n\n\n\n\n\n\n\n\n\n\n\nstar_rating\ncontent_rating\ngenre\nduration\nmajor_hit\nhigh_range_rev\n\n\n\n\n9.3\nR\nCrime\n142\nno\n2345444803\n\n\n9.2\nR\nCrime\n175\nno\n2182412593\n\n\n9.1\nR\nCrime\n200\nno\n1604872807\n\n\n9\nPG-13\nAction\n152\nno\n284317976\n\n\n8.9\nR\nCrime\n154\nyes\n1791932201\n\n\n\n\n\n\nExercise 2\nConvert the major_hit column to 1/0’s. yes -&gt; 1 and no -&gt; 0. Again, there are several ways to accomplish this. Using our old friend np.where is probably the easiest though.\nThe first 5 rows of the resulting dataframe should like this\n\n\n\n\n\n\n\n\n\n\n\nstar_rating\ncontent_rating\ngenre\nduration\nmajor_hit\nhigh_range_rev\n\n\n\n\n9.3\nR\nCrime\n142\n0\n1925521976\n\n\n9.2\nR\nCrime\n175\n0\n178034987\n\n\n9.1\nR\nCrime\n200\n0\n2618541398\n\n\n9\nPG-13\nAction\n152\n0\n997115723\n\n\n8.9\nR\nCrime\n154\n0\n1173054364\n\n\n\n\n\n\nExercise 3\nConvert the content_rating column using label encoding. We’re using label encoding in this case because the movie ratings already have a natural ordering to them. We will replace each rating with a number in it’s natural ascending order.\nTo be more specific, here is how we will do it.\n\nG: 0\nPG: 1\nPG-13: 2\nR: 3\n\nA dictionary and the .map() method could be useful for this exercise. There are other ways of tackling this problem though. Be creative!\nThe first 5 rows of the resulting dataframe should look like\n\n\n\n\n\n\n\n\n\n\n\nstar_rating\ncontent_rating\ngenre\nduration\nmajor_hit\nhigh_range_rev\n\n\n\n\n9.3\n3\nCrime\n142\n0\n1925521976\n\n\n9.2\n3\nCrime\n175\n0\n178034987\n\n\n9.1\n3\nCrime\n200\n0\n2618541398\n\n\n9\n2\nAction\n152\n0\n997115723\n\n\n8.9\n3\nCrime\n154\n0\n1173054364\n\n\n\n\n\n\nExercise 4\nThe last column that we need to take care of is genre. We will use one hot encoding for this. Make sure to ONLY one hot encode the genre column!\nA useful function for one hot encoding is pd.get_dummies(). I recommend checking out the documentation.\nThe resulting dataframe should look like the following example; don’t worry if your high_range_rev column turned into scientific notation—Pandas does this sometimes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstar_rating\ncontent_rating\nduration\nmajor_hit\nhigh_range_rev\ngenre_Action\ngenre_Adventure\ngenre_Animation\ngenre_Biography\ngenre_Comedy\ngenre_Crime\ngenre_Drama\ngenre_Family\ngenre_Fantasy\ngenre_Horror\ngenre_Mystery\ngenre_Sci-Fi\ngenre_Thriller\ngenre_Western\n\n\n\n\n0\n9.3\n3\n142\n0\n1.92552e+09\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n9.2\n3\n175\n0\n1.78035e+08\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n9.1\n3\n200\n0\n2.61854e+09\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n9\n2\n152\n0\n9.97116e+08\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n8.9\n3\n154\n0\n1.17305e+09\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\n\nExercise 5\nRecreate this graph as best you can. You’ll need to use the original data that specifies the actual rating.\n\n\n\n\n\n\n\n\nAfter you have completed this skill builder with your team (or on your own) then compare your work to our script\n\n\n\n\n\nSee the script.",
    "crumbs": [
      "Skill Builders",
      "Project 5: Munging Data"
    ]
  },
  {
    "objectID": "Skill Builders/pandas_plotly.html",
    "href": "Skill Builders/pandas_plotly.html",
    "title": "Pandas and Plotly",
    "section": "",
    "text": "For this skill builder, we are exploring some important functions in the package of pandas and plotly-express. DS programming requires a lot of data wrangling. Using the proper functions, we can create concise and comprehensive codes. You should be exposed to a few functions through the readings this week.\nYou may want to at least scan the readings before beginning this task since this serves as an assessment of your understanding of the assigned readings. This should be able to be finished within 60 minutes. You should work through it on your own or in a group based in your professors instruction.",
    "crumbs": [
      "Skill Builders",
      "Project 1: Pandas and Plotly"
    ]
  },
  {
    "objectID": "Skill Builders/pandas_plotly.html#skill-builder",
    "href": "Skill Builders/pandas_plotly.html#skill-builder",
    "title": "Pandas and Plotly",
    "section": "",
    "text": "For this skill builder, we are exploring some important functions in the package of pandas and plotly-express. DS programming requires a lot of data wrangling. Using the proper functions, we can create concise and comprehensive codes. You should be exposed to a few functions through the readings this week.\nYou may want to at least scan the readings before beginning this task since this serves as an assessment of your understanding of the assigned readings. This should be able to be finished within 60 minutes. You should work through it on your own or in a group based in your professors instruction.",
    "crumbs": [
      "Skill Builders",
      "Project 1: Pandas and Plotly"
    ]
  },
  {
    "objectID": "Skill Builders/pandas_plotly.html#data-import",
    "href": "Skill Builders/pandas_plotly.html#data-import",
    "title": "Pandas and Plotly",
    "section": "Data Import",
    "text": "Data Import\nRun the following code to import the data we need for this skill builder:\n\n# package import\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\n\n# data import\nurl = 'https://raw.githubusercontent.com/vincentarelbundock/Rdatasets/master/csv/AER/Guns.csv'\ndf = pd.read_csv(url)\nMake sure the variable df is correctly assigned in your environment and finish the following exercises. You can read the documentation of the data on this page - https://vincentarelbundock.github.io/Rdatasets/doc/AER/Guns.html\n\n\nExercise 1\nOne of the first things we can do to a freshly imported data is to check its columns. This will help us understand the basic structure of the dataframe(table).\n\nUsing one line of code, select all the columns in dat, assign it to a variable called col_list.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nEvery dataframe has an attribute “columns”.\nAccessing this attribute will give you a list of all column names\n\n\n\nWe often want to know the dimension of a dataframe. How many columns are in the dataset? How many rows are in the dataset?\n\nUsing one line of code, show the number of columns and rows in df.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nEvery dataframe has an attribute “shape”.\nAccessing this attribute will give you the dimension of a datafarme\n\n\n\nNow run df.head(). It will print out the first 5 rows of data in df.\n\nJust from looking at the output, what column(s) seems to be redundant with the row number?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nThere is one column that serves as nothing but a row counter, that columns is redundant.\n\n\n\n\n\n\nExercise 2\nAfter a brief investigation of the data, we will clean up the data. By cleaning up, we are trying to filter down df so this only holds data we need. We will first get rid of the extra column we found in the previous excercise.\n\nUsing one line of code, drop the redundant column using the variable col_list (created in excercise 1)\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse drop().\nUnderstand what “axis” is as a parameter of drop().\nYour function should looks like this:\ndf.drop([col_list[_]], axis = _)\nfill the “_“’s with the correct values and assign the output to df.\n\n\n\nDon’t forget to save the changes in df. Run df.head() to make sure the column is dropped in df.\n\n\n\nExercise 3\nWe have filtered df vertically by dropping a column. Now we will try to filter df horizontally, meaning we will get rid of some the rows.\nWe can do that by applying a condition to df. A condition is an expression that can be evaluated as True/False. For example, 8 &gt; 5 is an expression that evaluates to be True. This is trivial because 8 will always be greater than 5.\nRun the code below:\n\nwhat is the difference between exp1 and exp2?\n\nexp1 = 8 &gt; 5\nexp2 = df.violent &lt; 300\n\n\n\n\n\n\nHint\n\n\n\n\n\nTry type() on else variable OR calling else variable.\n\n\n\nRun ths code below:\n\nBy putting df.violent &lt; 300, and the violent column from df into a dataframe, what is the relationship between the two columns?\n\nexp = pd.DataFrame({\"df.violent &lt; 300\" : exp2,\n                    \"violent value from dat\" : df.violent})\n\nexp\n\n\n\n\n\n\nHint\n\n\n\n\n\nTry computing df.violent[n] &lt; 300 and (df.violent &lt; 300)[n] where n is less than the number of row. The two expressions will always be the same as long as n is less than the number of rows.\n\n\n\n\nUsing query()to filter down the df so that it only contains the data for idaho\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nquery() takes in expressions and filters down data.\n\n\n\nDon’t forget to save the changes in df. Run df.shape() to make sure the there are 23 rows and 13 columns.\n\n\n\nExercise 4\nBesides filtering, we can manipulate the data by adding new data to it. By adding a new column to the data, we assign a new value to each row.\n\nUsing assign(), create a new column that show the ratio between murder rate and violent rate.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse assign()\nYou see get the ratio by computing this code:\ndf.murder/df.violent\n\n\n\n\n\n\nExercise 5\n\nCreate a scatter plot that shows the relationship between murder rate and violent rate for the state of Idaho. Your chart should show murder rate as the x-axis, violent as the y-axis.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nCan you mimic this plot while using Plotly-Express?\nhttps://altair-viz.github.io/gallery/scatter_tooltips.html https://plotly.com/python/line-and-scatter/",
    "crumbs": [
      "Skill Builders",
      "Project 1: Pandas and Plotly"
    ]
  },
  {
    "objectID": "Skill Builders/pandas_plotly.html#because-youre-extra",
    "href": "Skill Builders/pandas_plotly.html#because-youre-extra",
    "title": "Pandas and Plotly",
    "section": "Because You’re Extra",
    "text": "Because You’re Extra\n\nExercise 6\n\nUsing a line of code, filter down the data set so that it only shows the data in years between 1993 and 1997.\n\n\n\n\nExercise 7\n\nCreate a line chart that show prisoners numbers for the state of Idaho, Utah, and Oregon.\n\nYour chart should show year as the x-axis, prisoner as the y-axis, states as different colours, along with an appropriate title.\n\n\n\nExercise 8\n\nWithout using query(), finshed the data wrangling in question 2,5 and 6.\n\n\n\n\n\n\n\n\nAfter you have completed this skill builder with your team (or on your own) then compare your work to our script\n\n\n\n\n\nSee the script.",
    "crumbs": [
      "Skill Builders",
      "Project 1: Pandas and Plotly"
    ]
  },
  {
    "objectID": "Syllabus/competency.html",
    "href": "Syllabus/competency.html",
    "title": "DS 250 Competency",
    "section": "",
    "text": "We need skills not grades! Shifting Attention\n\n\nCompetency scale\nYou must complete all competency items at the level detailed to achieve the listed grade. You can request half-step adjustments if you fall slightly short on some elements and over on others.\nYou will need to provide a detailed description in your Course Goals Letter of the items you completed to support your grade request. The course goals letter is a reflection on your efforts and the competencies they align with as well as your reflection on achieving your goals for this course.\n\nExample Course Goals Letter (End):\nHere is my assessment of my efforts and the competencie they aligh with as well as my reflection on achieving my goals for this course:\n\nProjects 29 points (B-)\nProject Stretchs 1 (C)\nMid-project checkpoints 4 (B+)\nMethods & Calculations checkpoints 4 (C+)\nDS Community 3 (A)\nCoding challenge 3 (A/B)\nThe over-under of my work is a B/B+. I am requesting a B+ for the course.\n\n\nLeader (A)Supporter (B)Listener (C)Asleep (D)\n\n\n\nLeader (A)\n\n\n\n\n\n\n\n\nElement\nRequirement\nDescription\n\n\n\n\nProjects\n34 Points\n5 points per project\n\n\nProject Stretchs\nAt least 3\n3 projects all stretches\n\n\nMid-project checkpoints\n5 completed\nFull credit (online only)\n\n\nMethods & Calculations checkpoints\n6 completed\nAll 6 @ 100% Full Points\n\n\nDS Community\nAt least 3\n–\n\n\nCourse Goal Letter (End)\nsubmission\n–\n\n\nCoding challenge\nAt least 3\nScore is out of 4\n\n\n\n\n\n\n\nSupporter (B)\n\n\n\n\n\n\n\n\nElement\nRequirement\nDescription\n\n\n\n\nProjects\n29 Points\n5 points per project\n\n\nProject Stretchs\nAt least 2\n2 projects all stretches\n\n\nMid-project checkpoints\n3 completed\nFull credit (online only)\n\n\nMethods & Calculations checkpoints\n5 completed\n5 @ 100% Full Points\n\n\nDS Community\nAt least 2\n–\n\n\nCourse Goal Letter (End)\nsubmission\n–\n\n\nCoding challenge\nAt least 3\nScore is out of 4\n\n\n\n\n\n\n\nListener (C)\n\n\n\n\n\n\n\n\nElement\nRequirement\nDescription\n\n\n\n\nProjects\n24 Points\n5 points per project\n\n\nProject Stretchs\nAt least 1\n1 projects all stretches\n\n\nMid-project checkpoints\n3 completed\nFull credit (online only)\n\n\nMethods & Calculations checkpoints\n3 completed\n3 @ 100% Full Points\n\n\nDS Community\nAt least 1\n–\n\n\nCourse Goal Letter (End)\nsubmission\n–\n\n\nCoding challenge\nAt least 2\nScore is out of 4\n\n\n\n\n\n\n\nAsleep (D)\n\n\n\n\n\n\n\n\nElement\nRequirement\nDescription\n\n\n\n\nProjects\n14 Points\n5 points per project\n\n\nProject Stretchs\nNone\n0 projects all stretches\n\n\nMid-project checkpoints\n1 completed\nFull credit (online only)\n\n\nMethods & Calculations checkpoints\n2 completed\n2 @ 100% Full Points\n\n\nDS Community\nNone\n–\n\n\nCourse Goal Letter (End)\nNone\n–\n\n\nCoding challenge\nNone\nScore is out of 4\n\n\n\n\n\n\n\n\n\n\nCompetency elements\n\nProjectsQuiz CheckpointsCheckpoints Mid-Project (online only)DS CommunityCourse Goals LetterChallenge\n\n\n\nProjects (Questions|Tasks)\nEach of the 7 projects is worth 5 points. There is a draft submission due Wednesday (W) of the 2nd week of the project and the final draft due the following Saturday (S). This gives you 2 attempts to get full points on the project. There are 6 two-week projects (P0-P5) and 1 one-week project (P6). Typically no resubmissions are allowed once the project closes in Canvas. Make the changes and take advantage of the resubmit within this window.\nProjects 1-5 will have stretch questions. You must complete all the core qeustions first before attempting the stretch questions. You must complete all the stretch questions accurately for the project to count toward the competencies. The number in the competencies relating to Project Stretches is the number of projects with all the stretch questions completed.\n\nGrading Details\n\n1 point: Submission\n3 points: Submission of a good faith attempt with a statement of work quality.\n4 points: High-quality work that addresses each of the Questions and Tasks and a comment in Canvas of your statement of work quality.\n5 points: Addressed issues and completion of resubmission (if needed).\n\n\n\n\n\n\nCheckpoints (methods and calculations)\nThese Methods and Calculation Quizzes are in Canvas and they open when the project starts. They have unlimited attempts and remain open until the end of the semester. You must get a 100% on these quizzes for them to count toward the competencies.\n\nExamples\n\nFact-Finding Questions (Calculate descriptive summaries): Fact-finding questions help you with calculations that build into the Questions and Tasks of the project. These questions have clearly defined answers using Python calculations. You should expect 2-3 problems.\n\n\nExample: Using the top 10 airports in size, what is the average size?\nExample: What proportion of flights are delayed at the largest airport?\n\n\nHow the code works questions (Explaining the tools): This part could have direct answer questions or open-ended questions.\n\n\nExample (direct): What is the recommended function for arranging your data by a variable? What are the outputs after using &lt;FUNCTION&gt;?\nExample (open): Your client has shown some confusion about NumPy’s ‘nan’ handling in Python. Help them understand by answering the question, ‘How is missing data handled in Pandas?’\n\n\n\n\n\n\nCheckpoints (Mid-project status)\nThe mid-project checkpoint has a few questions. It opens the first day of the project and closes on the first Saturday of a 2 week project. It has the following questions.\n\nExamples\n\nThroughout this project, you have worked on a code. Record a video showing your code that is no more than 1 minute. This video must include: &gt; How long have you worked on this code? &gt; What is your code designed to do? &gt; What are, if any, the issues you’re facing? &gt; What questions or tasks have you checked off?\nSubmit your 1 minute video. &gt; You may share any additional notes with your teacher using the Canvas comment feature.\n\n\n\n\n\n\nData Science Community\nTo earn credit for the DS Community element you must complete tasks from the list below. At the end of the semester, you will be asked to report on how many tasks you completed and what you learned from them. See the Competency Scale above to determine how many you need to complete based on the grade you want.\n\nAttend Data Science Society at least once.\nSign up for an email newsletter that will teach you more about data science. Data Science Weekly or Data Elixir are good options.\nListen to a podcast episode about data science. Build a Career in Data Science has some excellent episodes.\nWatch a professional presentation on YouTube about data science. Be prepared to share the link and a summary of the video.\nReach out to someone who works in a data-related field and ask them for 15 minutes of their time. Use this time to conduct an “informational interview” and learn more about their responsibilities and career path.\nResearch and apply to at least 5 data-related jobs or internships.\n\n\n\n\n\nFinishing the semester\nSubmit a Course Goals Letter (End) that includes what you have learned from this class, the next data science course you plan on taking, and the final grade that you are requesting based on the work you have submitted compared to the competencies above.\n\n\n\n\nCoding challenge\nWe will have a timed (60 min) coding challenge on the ultimate or penultimate day of class. This is not a traditional exam and is similar to the projects all semester in size and scope but is accumulative. It will cover the general techniques that we have been practicing throughout the course. You will rely on your code from the projects and the methods and calculations checkpoints to complete the challenge.\nWe expect to have a few practice challenges throughout the semester. We will score the coding challenge on a four-point scale.\n\n1 point: At least you tried.\n2 points: You have learned some items from the course, but your work in the coding challenge is deficient.\n3 points: Your submission uses proper coding techniques and addresses the objective.\n4 points: Exceptional work. Your code can be used as a solution to share with others.\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Syllabus",
      "Competency"
    ]
  },
  {
    "objectID": "Workbooks/wb5.html",
    "href": "Workbooks/wb5.html",
    "title": "Project 5 Workbook",
    "section": "",
    "text": "Project 5 WorkBook\nUnder Construction\n\n\n\n\n Back to top",
    "crumbs": [
      "Workbooks",
      "Project 5"
    ]
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Book Time with Us",
    "section": "",
    "text": "Book an office hour slot with your teacher.  Find office hours for all the Data Science Faculty Faculty.\n\n\n\n Back to top"
  },
  {
    "objectID": "materials.html",
    "href": "materials.html",
    "title": "Course Material",
    "section": "",
    "text": "We will be relying on a few resources for this course. You will find the pertinant readings attached to each of the projects. Those readings will be culled from:\n\nPython for Data Science: A port of R for Data Science using the Python packages pandas and Altair.\npandas User Guide\nAltair User Guide\nPlotly Express\nscikit-learn learn User Guide\nscikit-learn tutorials\nPython Data Science Handbook\nA Whirlwind Tour of Python\nSQL\n\nWes McKinney’s pandas code for his book Python for Data Analysis is a useful reference as well: https://github.com/wesm/pydata-book\n\n\n\n Back to top",
    "crumbs": [
      "Materials",
      "Course Materials"
    ]
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Course Setup",
    "section": "",
    "text": "How to get your computer setup?\nThe left navigation bar is sequenced for the setup process:\nIf you want to use a AWS Cloud Virtual Machine (VM) instead of your laptop and install Python and VS Code on that VM, start here:\n\nCreate an AWS EC2 VM (optional)\n\nThis course uses Slack as our main tool for communication and collaboration. You will need to install Slack on your laptop and mobile device. You will also need to create a Slack account using your BYU-Idaho email. If you have an existing account, you can either change your email to the BYU-Idaho email or you can create a new account.\n\nInstall Slack on your laptop:\n\nWindows\n\nMac\n\n\nInstall Slack on your Cell:\n\nAndroid\n\niPhone\n\nCreate a Slack Account using your BYU-Idaho email:\n\nSlack\n\n\nIf you want to use Python and VS code on your own laptop, start here:\n\nInstall Python\nInstall Python Libraries\n\nInstall VS Code\n\nInstall Quarto\n\nInstall Git & GitHub  \n\nYou will need to use SQL in Week 6:\n\nSQL setup and test\n\nUsing Copilot (AI)?\n\nCopilot is a micro-assistant that helps you write better code. It is a VS Code extension that uses AI built off Chat GPT-4. With your student GitHub account, you can use Copilot for free. It helps you write code faster and with fewer errors. It is not perfect, but it is a great tool to help you learn to code.\n\n\nThis course is designed with a Core and Stretch for each project. It is expected that you acomplish the Core without the use of Copilot. The Stretch is where you should use Copilot. Without using Copilot, the stretch questions will be more challenging. The goal is to help you learn to code and to use Copilot as a tool to help you learn, not to do the work for you.\n\n\nInstall GitHub Copilot (required)\n\nPrefer an Open Source AI tool try Llama?\n\nLlama is an AI-powered code editor that helps you write better code. It is a VS Code extension that uses AI built off Chat GPT-4. With your student GitHub account, you can use Llama for free. It helps you write code faster and with fewer errors. It is not perfect either, but it is a great tool to help you learn to code.\n\n\nInstall Llama (optional)\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Setup"
    ]
  },
  {
    "objectID": "slides.html",
    "href": "slides.html",
    "title": "Slides",
    "section": "",
    "text": "About this site\n\n\n\n Back to top"
  },
  {
    "objectID": "workbooks.html",
    "href": "workbooks.html",
    "title": "Workbooks",
    "section": "",
    "text": "Published Workbooks\n\nProject 0\n\nProject 1\n\nProject 2\n\nProject 3\nProject 4\nProject 5\n\nProject 6\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Workbooks",
      "Course Workbooks"
    ]
  },
  {
    "objectID": "faq.html",
    "href": "faq.html",
    "title": "Frequently Asked Questions",
    "section": "",
    "text": "“What do you mean by data science programming?”\nMost likely, you have had 1-2 courses of programming before you have taken DS 250. Unlike traditional computer science courses, DS 250 uses Python in an interactive mode instead of building programs. The data provider usually has some big questions that need answering; However, there are hundreds of little issues and responses along the way. We use programming to facilitate this investigation.\nThere are similarities with User Experience Designers. In our case, we don’t get to ask users about their experience. We use programming to ask data about its background, and each data set has its own history. We want our analysis to mold to that experience. You can think of data science programming like a first date with your data. You can’t write one long program nieve of the issues and nuances each living data set provides.\n\n\n\n“How does DS 250 compare to DS 350?”\nThe two courses have similarities. You could think of DS 250 as an introduction to data wrangling and visualization. Both classes use real-world data and are built around data science projects. There are some critical differences between the two courses.\n\nIn this course, we use Python, and DS 350 uses R.\nWe are introducing the principles of data science programming in DS 250.\nThe course is only 2-credits.\nDS 250 is intended to introduce visualization, wrangling, and modeling.\n\n\n\n\nfaq “How does DS 250 prepare me for DS 350 and CSE 450?”\nYou will be comfortable with interactive programming and have an introduction to the principles of data formats for data science applications. You will be introduced to principles related to machine learning, data wrangling, and data visualization.\n\n\n\n“What programming languages do we use in this course?”\nThe course is done using Python. We focus on the pandas and Altair packages.\n\n\n\n“What are the prerequisites for this course?”\nUsing the new courses at BYU-I, the prerequisite is CSE 110. However, if you have experience programming from other classes, you most likely are prepared for this course.\n\n\n\n“Why Python instead of R?”\nThe computer science and software engineering programs at BYU-I use Python as their foundational courses. The standard student will have some experience with Python before DS 250. Python is an essential programming language for data scientists, and we already have DS350, which is taught in R.\n\n\n\n“What is pandas?”\npandas is the foundational data science package in Python. If you are using tabular data you will be in pandas.\n\n\n\n“Why are we using Altair instead of Seaborn or Matplotlib?”\nMatplotlib was the first visualization package to gain a following in Python. Seaborn is built on top of Matplotlib. Many data scientists use both in their work—neither leverage the grammar of graphics as developed by Leland Wilkinson. Altair is built on Vega-Lite, which uses the Vega visualization grammar. It is declarative and actively developed. We expect that it will become the predominant visualization package in Python https://youtu.be/FytuB8nFHPQ and https://youtu.be/vTingdk_pVM.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Workbooks/wb0.html",
    "href": "Workbooks/wb0.html",
    "title": "Project 0 Workbook",
    "section": "",
    "text": "The data science lab is a resource you can use in person, online, and in Slack.\n\n\n\n\n\nCourse Setup ::: {.callout-note collapse=“true”} ## Note on the Intro Chapter in Python for DataScience\nDon’t follow the setup for pyton in the book use the Course Setup instead\nTesting running the instructional template to make sure your setup is correct :::\n\n\n\n\n\n\nLearn about Lets-Plot\n\n:::",
    "crumbs": [
      "Workbooks",
      "Project 0"
    ]
  },
  {
    "objectID": "Workbooks/wb0.html#tutoring-lab-info",
    "href": "Workbooks/wb0.html#tutoring-lab-info",
    "title": "Project 0 Workbook",
    "section": "",
    "text": "The data science lab is a resource you can use in person, online, and in Slack.",
    "crumbs": [
      "Workbooks",
      "Project 0"
    ]
  },
  {
    "objectID": "Workbooks/wb0.html#setup-python-vs-code-and-quarto",
    "href": "Workbooks/wb0.html#setup-python-vs-code-and-quarto",
    "title": "Project 0 Workbook",
    "section": "",
    "text": "Course Setup ::: {.callout-note collapse=“true”} ## Note on the Intro Chapter in Python for DataScience\nDon’t follow the setup for pyton in the book use the Course Setup instead\nTesting running the instructional template to make sure your setup is correct :::",
    "crumbs": [
      "Workbooks",
      "Project 0"
    ]
  },
  {
    "objectID": "Workbooks/wb0.html#learn-about-lets-plot",
    "href": "Workbooks/wb0.html#learn-about-lets-plot",
    "title": "Project 0 Workbook",
    "section": "",
    "text": "Learn about Lets-Plot\n\n:::",
    "crumbs": [
      "Workbooks",
      "Project 0"
    ]
  },
  {
    "objectID": "Workbooks/wb4.html",
    "href": "Workbooks/wb4.html",
    "title": "Project 4 Workbook",
    "section": "",
    "text": "The data science lab is a resource you can use in person, online, and in Slack.",
    "crumbs": [
      "Workbooks",
      "Project 4"
    ]
  },
  {
    "objectID": "Workbooks/wb4.html#tutoring-lab-info",
    "href": "Workbooks/wb4.html#tutoring-lab-info",
    "title": "Project 4 Workbook",
    "section": "",
    "text": "The data science lab is a resource you can use in person, online, and in Slack.",
    "crumbs": [
      "Workbooks",
      "Project 4"
    ]
  },
  {
    "objectID": "Projects/project_1.html",
    "href": "Projects/project_1.html",
    "title": "Project 1: What’s in a Name?",
    "section": "",
    "text": "Walkthrough\n\n\n\nBackground\n\n\n\n\n\n\nNote\n\n\n\n\n\nWe will complete six projects during the semester that each take about two weeks (four days of class). On average, a student will spend 2 hours outside of class per hour in class to complete the assigned readings, submit any Canvas items, and complete the project (for a total of 8 hours per project). The instruction for each project will be structured into sections as written on this page.\nThis first Background section provides context for the project. Make sure you read the background carefully to see the big picture needs and purpose of the project.\n\n\n\nEarly in prehistory, some descriptive names began to be used again and again until they formed a name pool for a particular culture. Parents would choose names from the pool of existing names rather than invent new ones for their children.\nWith the rise of Christianity, certain trends in naming practices manifested. Christians were encouraged to name their children after saints and martyrs of the church. These early Christian names can be found in many cultures today, in various forms. These were spread by early missionaries throughout the Mediterranean basin and Europe.\nBy the Middle Ages, the Christian influence on naming practices was pervasive. Each culture had its pool of names, which were a combination of native names and early Christian names that had been in the language long enough to be considered native. [ref]\n\n\nClient Request\nThis csv file contains the number of times a name was given to a child in a specific year. The Client has a passion for names throughout history. They would like to know how the usage of names has changed over time. They are particularly interested in the names Mary, Martha, Peter, and Paul. They would also like to know how the usage of a name from a famous movie has changed over time.\n\n\nData\n\n\n\n\n\n\nNote\n\n\n\n\n\nEvery data science project should start with data, and our class projects are no different. Each project will have ‘Download’ and ‘Information’ links like the ones below.\n\n\n\nDownload: names_year.csv\nInformation: data.md\n\n\nReadings\n\n\n\n\n\n\nNote\n\n\n\n\n\nThe Readings section will contain links to reading assignments that are required for each project, as well as optional references. Remember that you are reading this material to build skills. Take the time to comprehend the readings and the skills contained within.\nWe recommend reading through the assigned material once for a general understanding before the first day of each project. You will reread and reference the material multiple times as you complete the project.\n\n\n\n\nP4DS: CH7 Workflow: Writing Code (Skim)\nP4DS: CH8 Data Import (Read)\nP4DS: CH14 Graphics for Communication (Read)\n\n\nOptional References\n\nThe query method\nEasy Data Visualiztation for Tidy Data with Lets-Plot\nExplore Your Data with Lets-Plot\n\n\n\n\nQuestions and Tasks (Core)\n\n\n\n\n\n\nNote\n\n\n\n\n\nThis section lists the questions and tasks that need to be completed for the project. Your work on the project must be compiled into a report, pushed to GitHub and a URL submitted in Canvas by the weekend following the last day of material for the project.\nThere are two types of questions: Core and Stretch. Core questions are required for each project. The course syllabus competencies requires specic a number of projects having all the Stretch questions achived based on your goals for the grade level you are seeking.\n\n\n\nIn the DS 250 folder of the Course Work Portfolio, edit the Project1.qmd quarto file to build a report that includes the following:\nFor Project 1 the answer to each question should include a chart and a written response. The years labels on your charts should not include a comma. At least two of your charts must include reference marks.\n\nHow does your name at your birth year compare to its use historically?\n\nIf you talked to someone named Brittany on the phone, what is your guess of his or her age? What ages would you not guess?\n\nMary, Martha, Peter, and Paul are all Christian names. From 1920 - 2000, compare the name usage of each of the four names in a single chart. What trends do you notice?\n\nThink of a unique name from a famous movie. Plot the usage of that name and see how changes line up with the movie release. Does it look like the movie had an effect on usage?\n\n\n\nQuestions and Tasks (Stretch)\nHere is an example Stretch question(s) for this project. Your instructor may assign different Stretch question(s). You must comment in Canvas when submitting your project if you completed any of the Stretch questions.\n\nReproduce the chart Elliot using the data from the names_year.csv file.\n\n\n\n\nSubmission:\n\n\n\n\n\n\nNote\n\n\n\n\n\nWhen you have completed the report, you will need to follow this process to submit your work:\n\nBefore you begin you must Click the link in the email from GitHub to join your class group in the BYUI-math-dept org in GitHub. (If you have not received an email, please DM your teacher in Slack, this invitation does expire after 7 days so please dont delay in accepting the invitation)\nHave the Course Work Portfolio open in VS Code\nClick Termanal in VS Code and open a new terminal\nIn the terminal, type quarto render and press enter\n\nThis will render the entire course work portfolio into HTML files\nThis will move all those files into the docs folder\nThis can take a few minutes to complete\nIf there is an error in any cell of the quarto files, the rendering will stop and you will need to fix the error before rendering again (if you get stuck post your error in Slack)\n\nOnce the report is rendered, open the GitHub Desktop application\nType a summary of the changes in the Summary box\nClick Commit to main blue button in the bottom left corner\nClick Push origin blue button in the middle right of the screen\n\n\n\n\n\n\nDeliverables:\n\n\n\n\n\n\nNote\n\n\n\n\n\nDeliverables are “the quantifiable goods or services that must be provided upon the completion of a project”. In this class the deliverable for each project is a GitHub published report created using Quarto files. This final section will be the same for each project.\n\n\n\nUse this template to submit your Client Report. The template has two sections:\n\nA short elevator pitch that highlights key values or metrics from the results. Describing these key insights to interest or hook the reader to want to read more about your work. The writing style should be more technical with some creative elements. Do not summarize what you did.\n\nAnswers to the questions | tasks. Each should include a written description of your results, code cells with comments, charts and/or tables.\n\nA short summary of work must be submitted in the comments in Canvas wwhen you submit the URL. Rate your own work on a scale of 1-5. 1 being poor and 5 being excellent. Include a short description of why you rated your work the way you did.\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nYour report should be written in quarto markdown files and pushed to GitHub. Submit a URL of the rendered project in Canvas. (Do not submit the URL to the GitHub .qmd file)\n\n\n\n\n\nFeedback:\n\n\n\n\n\n\nNote\n\n\n\n\n\nYou will recieve feedback and/or coaching notes in the form of a GitHub issue. You will need to address the feedback, re-render and resubmit the project, and mark the GitHub issue as closed.\n\n\n\n\n\nResubmission:\n\n\n\n\n\n\nNote\n\n\n\n\n\nYou will have one opportunity to resubmit the project after you have received feedback. The window for the resubmission will be open through the Wednesday following the due date of the project. Therefore it is recomended that you turn in a draft of the project early on the Thursday before the due date to ensure you have time to address any feedback and resubmit the project. It is acceptable to turn in a draft that is only 80% complete. This will allow you to get feedback on the majority of the project and then focus on the final details. The closer to that Thursday you turn in the draft the more feedback and coaching you will recieve.\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Projects",
      "Project 1: What's in a Name?"
    ]
  },
  {
    "objectID": "Projects/project_4.html",
    "href": "Projects/project_4.html",
    "title": "Project 4: Can You Predict That?",
    "section": "",
    "text": "Walkthrough\n\n\n\nBackground\nThe clean air act of 1970 was the beginning of the end for the use of asbestos in home building. By 1976, the U.S. Environmental Protection Agency (EPA) was given authority to restrict the use of asbestos in paint. Homes built during and before this period are known to have materials with asbestos YOu can read more about this ban.\nThe state of Colorado has a large portion of their residential dwelling data that is missing the year built and they would like you to build a predictive model that can classify if a house is built pre 1980.\nColorado gave you home sales data for the city of Denver from 2013 on which to train your model. They said all the column names should be descriptive enough for your modeling and that they would like you to use the latest machine learning methods.\n\n\nClient Request\nThe Client is a state agency in Colorado that is responsible for the health and safety of its residents. They have a large portion of their residential dwelling data that is missing the year built and they would like you to build a predictive model that can classify if a house is built pre 1980.\n\n\nData\nDownload: dwellings_ml.csv (ml ready)\nOptional Data: dwellings_neighborhoods_ml.csv (ml ready)\nInformational Data: dwellings_denver.csv (not cleansed)\nInformation: Data description\n\n\nReadings\n\nMachine Learning Introduction (Skimi)\nA visual introduction to machine learning (Read)\nP4DS: CH22 Joins (Read)\nHow to choose a good evaluation metric for your Machine learning model (Skim)\n\n\nOptional References\n\nDecision Tree Classification in Python\n\nBoosted algorithms in scikit-learn\nscikit-plot package\n\n\n\n\nQuestions and Tasks (Core)\n\nCreate 2-3 charts that evaluate potential relationships between the home variables and before1980. Explain what you learn from the charts that could help a machine learning algorithm.\nBuild a classification model labeling houses as being built “before 1980” or “during or after 1980”. Your goal is to reach or exceed 90% accuracy. Explain your final model choice (algorithm, tuning parameters, etc) and describe what other models you tried.\nJustify your classification model by discussing the most important features selected by your model. This discussion should include a feature importance chart and a description of the features.\nDescribe the quality of your classification model using 2-3 different evaluation metrics. You also need to explain how to interpret each of the evaluation metrics you use.\n\n\n\nQuestions and Tasks (Stretch)\nHere is an example Stretch question(s) for this project. Your instructor may assign different Stretch question(s). You must comment in Canvas when submitting your project if you completed any of the Stretch questions.\n\nRepeat the classification model using 3 different algorithms. Display their Feature Importance, and Decision Matrix. Explian the differences between the models and which one you would recommend to the Client.\nJoin the dwellings_neighborhoods_ml.csv data to the dwelling_ml.csv on the parcel column to create a new dataset. Duplicate the code for the stretch question above and update it to use this data. Explain the differences and if this changes the model you recomend to the Client.\nCan you build a model that predicts the year a house was built? Explain the model and the evaluation metrics you would use to determine if the model is good.\n\n\n\nSubmission:\n\n\n\n\n\n\nNote\n\n\n\n\n\nWhen you have completed the report, you will need to follow this process to submit your work:\n\nBefore you begin you must Click the link in the email from GitHub to join your class group in the BYUI-math-dept org in GitHub. (If you have not received an email, please DM your teacher in Slack, this invitation does expire after 7 days so please dont delay in accepting the invitation)\nHave the Course Work Portfolio open in VS Code\nClick Termanal in VS Code and open a new terminal\nIn the terminal, type quarto render and press enter\n\nThis will render the entire course work portfolio into HTML files\nThis will move all those files into the docs folder\nThis can take a few minutes to complete\nIf there is an error in any cell of the quarto files, the rendering will stop and you will need to fix the error before rendering again (if you get stuck post your error in Slack)\n\nOnce the report is rendered, open the GitHub Desktop application\nType a summary of the changes in the Summary box\nClick Commit to main blue button in the bottom left corner\nClick Push origin blue button in the middle right of the screen\n\n\n\n\n\n\nDeliverables:\nUse this template to submit your Client Report. The template has two sections:\n\nA short elevator pitch that highlights key values or metrics from the results. Describing these key insights to interest or hook the reader to want to read more about your work. The writing style should be more technical with some creative elements. Do not summarize what you did.\n\nAnswers to the questions | tasks. Each should include a written description of your results, code cells with comments, charts and/or tables.\n\nA short summary of work must be submitted in the comments in Canvas wwhen you submit the URL. Rate your own work on a scale of 1-5. 1 being poor and 5 being excellent. Include a short description of why you rated your work the way you did.\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Projects",
      "Project 4: Can You Predict That?"
    ]
  },
  {
    "objectID": "Projects/project_6.html",
    "href": "Projects/project_6.html",
    "title": "Project 6: Git Your DS Portfolio Online",
    "section": "",
    "text": "Walkthrough\n\n\n\nBackground\nGitHub is an online platform where data scientists and developers can communicate and share work. It has also morphed into a tool to house all your work in a portfolio. Think about an Art student and how they have to develop their portfolio of various skills they have across the art classes. Similarly you will want to showcase you skillset across the Data Science skillsets.\nAs students, you will want to curate your creative work on GitHub using a program called Git. GitHub is the place to share your original work, not your homework assignments. The reading assignments will dive deaper into what to include in your portfolio and what not to include.\nMany people store their personal websites, blogs, and project websites on GitHub. Our textbook and course are hosted on GitHub, and you can see J. Hathaway’s or Ryan Hafen’s personal Data Science websites that are hosted on GitHub as well.\nFor this project, you will be making a public website that is a data science portfolio that will be hosted on GitHub. Your Resume (from project 0) will be one section of your portfolio/website. You should also post Data Science Society projects, personal projects, and any other data science related work you have done outside of class.\n\n\nData\nPortfolio: BYUI Data Science Portfolio\n\n\nReadings\n\nGit and GitHub for DS\nPull and Merge Forks on GitHub\nNew to Git and GitHub? This Essential Beginners Guide is for you\nGit vs. GitHub: What is the difference between them?\nUsing Version Control in VS Code\nGit in Visual Studio Code video\n\n\n\nPortfolio Resources\n\nHow to Modify a Quarto Website\nHow to Create a Compelling GitHub Portfolio\nHow to Create a Professional Portfolio on GitHub\nData Science Portfolios That Will Get You the Job\n4 Data Science Portfolio Projects You Need to Create\nExample 1 - Data Science Portfolio\nExample 2 - Data Science Portfolio\n\n\n\nQuestions and Tasks\n\nGit a Data Science Portfolio in GitHub (main page)\n\nUse the Portfolio Template on your Githhub root directory\n\nNavigate to the Data Science Portfolio repo in GitHub.\nClick the Green Button Use this template and select Create a new repository\n\nClick include all branches checkbox, this will include the gh-pages branch\n\nSelect yourself as the Owner\n\nName the repository as username.github.io where the username is your username on GitHub (Note: If the username part of the repository doesn’t exactly match your username, it won’t work, so make sure to get it right.)\nClick the Green Button Create repository\n\n\nCreate a new branch gh-pages if you forgot to check the include all branches box (skip otherwise)\n\nClick the Branch: main button then view all branches\n\nClick the New Branch button\n\nName the branch gh-pages and click the Green Button Create new branch\n\n\nModify Pages Settings for Build and deployment from main to gh-pages:\n\nClick the Settings tab\n\nScroll down to the Pages section in the left hand menu\n\nLocate the Build and deployment section and change Branch from main to gh-pages and leave the right side as /root\n\n\nClone the repository to your computer\n\nClick the &lt;&gt; Code menu\nClick the Green Button &lt;&gt; Code and select Open with GitHub Desktop\n\nClick the Button Open in Visual Studio Code \n\nUpdate the _quarto.yml file\n\nChange the title to your name\nChange the repo-rul to a brief description of your portfolio\nChange the page-footer left: to your name\nChange the page footer href: to your LinkedIn profile link\nScroll to the bottom and change the theme light: and/or dark: to another theme (optional)\n\nPush the changes to GitHub via GitHub Desktop\n\nMake sure your current repo in the top left is username.github.io\n\nType a commit message and click the Blue Button Commit to main\n\nClick the Blue Button Push origin\n\n\nConfirm the GitHub Actions are working\n\nNavigate to the repo in GitHub and click on the Actions tab\n\nConfirm the Update _quarto.yml is working by the yellow circle turning to a green check circle (Note: this can take 3-5min)\n\n\nFix the main page loading the ReadMe.md file\n\nRun quarto publish gh-pages in the terminal of VS Code\n\n\nGit your Resume in your Portfolio\n\nMove your resume from Project 0 into the Portfolio by replacing the resume.qmd file\nPush your results to GitHub with GitHub Desktop.\n\n\n\n\nDeliverables:\n\nComplete the questions\n\nSubmit a URL link to your portfolio.\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Projects",
      "Project 6: Git Your Portfolio Online"
    ]
  },
  {
    "objectID": "Setup/slack_setup.html",
    "href": "Setup/slack_setup.html",
    "title": "VS Code for Data Science",
    "section": "",
    "text": "Intro to Slack\n\n\n\nDownload and Install Slack\nUse the link below to download and install the latest version of Slack (both desktop and mobile):\n\nSlack for Windows\nSlack for Mac\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Setup",
      "Slack"
    ]
  },
  {
    "objectID": "Syllabus/syllabus.html",
    "href": "Syllabus/syllabus.html",
    "title": "DS 250 Syllabus",
    "section": "",
    "text": "Most people would sooner die than think, and most of them do.\n-Bertrand Russell-",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "Syllabus/syllabus.html#footnotes",
    "href": "Syllabus/syllabus.html#footnotes",
    "title": "DS 250 Syllabus",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://medium.com/@nikhilbd/what-makes-a-good-data-scientist-engineer-a8b4d7948a86#.jr80wl98y. I suppose some of you are just taking this class because your degree says you can, and it fits in your schedule. If so, we should chat to make sure this is the right class for you.↩︎\nhttps://arxiv.org/ftp/arxiv/papers/1612/1612.07140.pdf. You will see this pattern in DS 350, DS 460, and Math 488. It will progressively get more realistic.↩︎\nWe do expect that this is not your first experience with Python and VS Code. If you have done other programming courses, you should be able to succeed in this course. If you have any questions, please ask.↩︎\nMaking the right checklists can be difficult. Bad checklists could fall in the following categories – vague and imprecise; too long; hard to use; impractical; too pedantic. Useful checklists are precise, efficient, easy to use and understand. This is the first time this course has been offered, so we will have to work together to ensure the requirements are reasonable.↩︎",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "Projects/project_3.html",
    "href": "Projects/project_3.html",
    "title": "Project 3: Finding Relationships in Baseball",
    "section": "",
    "text": "Walkthrough\n\n\n\nSQL Refresher\n\n\n\nBackground\nWhen you hear the word “relationship” what is the first thing that comes to mind? Probably not baseball. But a relationship is simply a way to describe how two or more objects are connected. There are many relationships in baseball such as those between teams and managers, players and salaries, even stadiums and concession prices.\nThe graphs on Data Visualizations from Best Tickets show many other relationships that exist in baseball.\n\n\nClient Request\nFor this project, the Client wants SQL queries that they can use to retrieve data for use on their website without needing Python. They would also like to see the results in Plotly Express charts.\n\n\nData\nDownload: lahmansbaseballdb\nInformation: Lahman Data Dictionary\nSetup Instructions: See SQL Setup\n\n\nReadings\n\nSQL Setup and References (Read)\nSQL for Data Science (Read)\n\n\nOptional References\n\nWhy SQL is beating NoSQL, and what this means for the future of data\n\n\n\n\nQuestions and Tasks (Core)\n\nWrite an SQL query to create a new dataframe about baseball players who attended BYU-Idaho. The new table should contain five columns: playerID, schoolID, salary, and the yearID/teamID associated with each salary. Order the table by salary (highest to lowest) and print out the table in your report.\nThis three-part question requires you to calculate batting average (number of hits divided by the number of at-bats)\n\nWrite an SQL query that provides playerID, yearID, and batting average for players with at least 1 at bat that year. Sort the table from highest batting average to lowest, and then by playerid alphabetically. Show the top 5 results in your report.\n\nUse the same query as above, but only include players with at least 10 at bats that year. Print the top 5 results.\n\nNow calculate the batting average for players over their entire careers (all years combined). Only include players with at least 100 at bats, and print the top 5 results.\n\nPick any two baseball teams and compare them using a metric of your choice (average salary, home runs, number of wins, etc). Write an SQL query to get the data you need, then make a graph using Plotly Express to visualize the comparison. What do you learn?\n\n\n\nQuestions and Tasks (Stretch)\nHere is an example Stretch question(s) for this project. Your instructor may assign different Stretch question(s). You must comment in Canvas when submitting your project if you completed any of the Stretch questions.\n\nAdvanced Salary Distribution by Position (with Case Statement):\n\nWrite an SQL query that provides a summary table showing the average salary for players in each position (e.g., pitcher, catcher, outfielder) across all years. Include the following columns:\n\nposition\naverage_salary\ntotal_players\nhighest_salary\n\nThe highest_salary column should display the highest salary ever earned by a player in that position. If no player in that position has a recorded salary, display “N/A” for the highest salary.\nAdditionally, create a new column called salary_category using a case statement:\n\nIf the average salary is above $1 million, categorize it as “High Salary.”\n\nIf the average salary is between $500,000 and $1 million, categorize it as “Medium Salary.”\n\nOtherwise, categorize it as “Low Salary.”\n\nOrder the table by average salary in descending order.\nPrint the top 10 rows of this summary table.\n\nAdvanced Career Longevity and Performance (with Subqueries):\n\nCalculate the average career length (in years) for players who have played at least one game. Then, identify the top 10 players with the longest careers (based on the number of years they played). Include their:\n\nplayerID\nfirst_name\nlast_name\ncareer_length\n\nThe career_length should be calculated as the difference between the maximum and minimum yearID for each player.\n\n\n\n\nSubmission:\n\n\n\n\n\n\nNote\n\n\n\n\n\nWhen you have completed the report, you will need to follow this process to submit your work:\n\nBefore you begin you must Click the link in the email from GitHub to join your class group in the BYUI-math-dept org in GitHub. (If you have not received an email, please DM your teacher in Slack, this invitation does expire after 7 days so please dont delay in accepting the invitation)\nHave the Course Work Portfolio open in VS Code\nClick Termanal in VS Code and open a new terminal\nIn the terminal, type quarto render and press enter\n\nThis will render the entire course work portfolio into HTML files\nThis will move all those files into the docs folder\nThis can take a few minutes to complete\nIf there is an error in any cell of the quarto files, the rendering will stop and you will need to fix the error before rendering again (if you get stuck post your error in Slack)\n\nOnce the report is rendered, open the GitHub Desktop application\nType a summary of the changes in the Summary box\nClick Commit to main blue button in the bottom left corner\nClick Push origin blue button in the middle right of the screen\n\n\n\n\n\n\nDeliverables:\nUse this template to submit your Client Report. The template has two sections:\n\nA short elevator pitch that highlights key values or metrics from the results. Describing these key insights to interest or hook the reader to want to read more about your work. The writing style should be more technical with some creative elements. Do not summarize what you did.\n\nAnswers to the questions | tasks. Each should include a written description of your results, code cells with comments, charts and/or tables.\n\nA short summary of work must be submitted in the comments in Canvas wwhen you submit the URL. Rate your own work on a scale of 1-5. 1 being poor and 5 being excellent. Include a short description of why you rated your work the way you did.\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Projects",
      "Project 3: Finding Relationships in Baseball"
    ]
  },
  {
    "objectID": "Setup/git_github_setup.html",
    "href": "Setup/git_github_setup.html",
    "title": "Git and GitHub for DS",
    "section": "",
    "text": "Note: Changes have been made to this course since the video was recorded. The video is still relevant and will be updated in the future to reflect the changes. For conflicts of the video and the text, the text is the most current.\n\n\n\nSetup Git and GitHub\n\nGitHub will be used for publishing all projects\n\nInstall git on your computer\n\nWindows Installation Note: Keep all settings as default during installation\nMac Installation\nNote: Mac users can install git using homebrew by running brew install git in the terminal. You will need to install homebrew first if you don’t have it\n\nCreate a GitHub Student Account with your BYUI-I email.\n\nUse an appropriate username. It will be the name of your public profile and website in Project 6.\n\nInstall GitHub Desktop\nGit a Course Work Portfolio in GitHub\n\nUse the Portfolio Template in Data Science GitHub repo\n\nNavigate to the Course Work Portfolio in GitHub\n\nClick the Green Button Use this template and select Create a new repository\n\nCheck the box to Include all branches\n\n\nSelect byui-math-dept as the Owner You should have been added to byui-math-dept by your teacher if you dont see it ask them to add you. This Org uses SSO with BYUI, if you dont have your BYUI email in your GitHub account you need to add it (account -&gt; settings -&gt; emails -&gt; add email address) If you have performed all these steps and still dont see it look for a badge above Repository template that states Single sign-on to see results in the byui-math-dept organization. Follow that link to sign in with SSO then you will be able to choose byui-math-dept as the Owner\n\n\nName the repository as your GitHub username + _ + your first semester and yearall lowercase (see example in image above)\n\nSelect Private as the type of Repo, then click Create Repository\n\n\nCreate a new branch gh-pages if you forgot to check the include all branches box (skip otherwise)\n\nClick the Branch: main button then view all branches\n\nClick the New Branch button\n\nName the branch gh-pages and click the Green Button Create new branch\n\n\nModify Pages Settings for Build and deployment from main to gh-pages:\n\nClick the Settings tab\n\nScroll down to the Pages section in the left hand menu\n\nLocate the Build and deployment section and change Branch from main to gh-pages and leave the right side as /root\n\n\nClone the repository to your computer\n\nClick the &lt;&gt; Code menu\nClick the Green Button &lt;&gt; Code and select Open with GitHub Desktop\n\n\nClick the Button Open in Visual Studio Code \n\nIf it asks for a username and password, this is because your GitHub Desktop is not logged in to your GitHub account via SSO. Log out of your account in GitHub Desktop, be logged in in your browser to GitHub and make sure you can access the byui-math-dept org where you cloned your new portfolio. Then repete these instructions. It will log you back in to GitHub Desktop but this with with the SSO credentials\n\nUpdate the _quarto.yml file\n\nChange the title to your name\nChange the repo-rul to a brief description of your portfolio\nChange the page-footer left: to your name\nChange the page footer href: to your LinkedIn profile link\nScroll to the bottom and change the theme light: and/or dark: to another theme (optional)\n\nPush the changes to GitHub via GitHub Desktop\n\nMake sure you have the correct repo selected in the top left\n\n\nType a commit message and click the Blue Button Commit to main\n\n\nClick the Blue Button Push origin\n\n\nConfirm the GitHub Actions are working\n\nNavigate to the repo in GitHub and click on the Actions tab\n\nConfirm the Update _quarto.yml is working by the yellow circle turning to a green check circle (Note: this can take 3-5min)\n\n\nFix the main page loading the ReadMe.md file\n\nRun quarto publish gh-pages in the terminal of VS Code\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Setup",
      "Git and Github"
    ]
  },
  {
    "objectID": "Course Materials/sql_page.html",
    "href": "Course Materials/sql_page.html",
    "title": "Introduction",
    "section": "",
    "text": "Structured Query Language (SQL) is a powerful tool for managing and manipulating relational databases. It is essential for data scientists, analysts, and anyone working with large datasets. This chapter will explore the importance of SQL, its applications, and provide example code format to illustrate its utility."
  },
  {
    "objectID": "Course Materials/sql_page.html#why-sql-is-important",
    "href": "Course Materials/sql_page.html#why-sql-is-important",
    "title": "Introduction",
    "section": "Why SQL is Important",
    "text": "Why SQL is Important\n\nData Management: SQL allows for efficient management of large volumes of data. It provides the means to create, read, update, and delete data in a relational database.\nData Retrieval: With SQL, you can perform complex queries to retrieve specific data from one or more tables, making it easier to analyze and draw insights.\nData Manipulation: SQL enables the manipulation of data through operations such as sorting, filtering, and aggregating. This is crucial for data cleaning and preprocessing.\nData Integration: SQL supports the integration of data from different sources, allowing for comprehensive data analysis.\nStandardization: SQL is a standardized language used by most relational database management systems (RDBMS), making it a versatile and essential skill for professionals in the field."
  },
  {
    "objectID": "Course Materials/sql_page.html#basic-sql-concepts",
    "href": "Course Materials/sql_page.html#basic-sql-concepts",
    "title": "Introduction",
    "section": "Basic SQL Concepts",
    "text": "Basic SQL Concepts\n\nimport sqlite3\nimport pandas as pd"
  },
  {
    "objectID": "Course Materials/sql_page.html#basic-sql-concepts-1",
    "href": "Course Materials/sql_page.html#basic-sql-concepts-1",
    "title": "Introduction",
    "section": "Basic SQL Concepts",
    "text": "Basic SQL Concepts\nTo illustrate the following SQL concepts, we will use the employees table with the following data:\nTable: employees\n\n\n\nid\nfirst_name\nlast_name\ndepartment\nsalary\n\n\n\n\n1\nAlice\nSmith\nHR\n60000\n\n\n2\nBob\nJohnson\nIT\n80000\n\n\n3\nCharlie\nLee\nSales\n55000\n\n\n4\nDavid\nKim\nHR\n75000\n\n\n5\nEva\nBrown\nIT\n65000\n\n\n6\nFrank\nWilson\nSales\n70000\n\n\n7\nGrace\nTaylor\nHR\n62000\n\n\n8\nHenry\nAnderson\nIT\n77000\n\n\n9\nIrene\nThomas\nSales\n53000\n\n\n10\nJack\nWhite\nHR\n58000\n\n\n11\nKaren\nHarris\nIT\n69000\n\n\n12\nLeo\nMartin\nSales\n50000\n\n\n13\nMia\nJackson\nHR\n64000\n\n\n14\nNoah\nLee\nIT\n72000\n\n\n15\nOlivia\nPerez\nSales\n68000\n\n\n16\nPaul\nYoung\nHR\n61000\n\n\n17\nQuinn\nKing\nIT\n76000\n\n\n18\nRachel\nScott\nSales\n57000\n\n\n19\nSam\nGreen\nHR\n63000\n\n\n20\nTina\nAdams\nIT\n81000\n\n\n\n\nSELECT and FROM\nThe SELECT statement is used to fetch data from a database, and the FROM clause specifies the table.\n-- Selecting all columns from a table\np = \"\"\"\n\nSELECT * \nFROM \n  employees;\n\n\"\"\"\n\npd.read_sql_query(p, con)\n\n\n\nid\nfirst_name\nlast_name\ndepartment\nsalary\n\n\n\n\n1\nAlice\nSmith\nHR\n60000\n\n\n2\nBob\nJohnson\nIT\n80000\n\n\n3\nCharlie\nLee\nSales\n55000\n\n\n4\nDavid\nKim\nHR\n75000\n\n\n5\nEva\nBrown\nIT\n65000\n\n\n6\nFrank\nWilson\nSales\n70000\n\n\n7\nGrace\nTaylor\nHR\n62000\n\n\n8\nHenry\nAnderson\nIT\n77000\n\n\n9\nIrene\nThomas\nSales\n53000\n\n\n10\nJack\nWhite\nHR\n58000\n\n\n11\nKaren\nHarris\nIT\n69000\n\n\n12\nLeo\nMartin\nSales\n50000\n\n\n13\nMia\nJackson\nHR\n64000\n\n\n14\nNoah\nLee\nIT\n72000\n\n\n15\nOlivia\nPerez\nSales\n68000\n\n\n16\nPaul\nYoung\nHR\n61000\n\n\n17\nQuinn\nKing\nIT\n76000\n\n\n18\nRachel\nScott\nSales\n57000\n\n\n19\nSam\nGreen\nHR\n63000\n\n\n20\nTina\nAdams\nIT\n81000\n\n\n\n-- Selecting specific columns\np = \"\"\"\n\nSELECT \n  first_name, \n  last_name, \n  salary \nFROM \n  employees;\n\n\"\"\"\n\npd.read_sql_query(p, con)\n\n\n\nid\nfirst_name\nlast_name\nsalary\n\n\n\n\n1\nAlice\nSmith\n60000\n\n\n2\nBob\nJohnson\n80000\n\n\n3\nCharlie\nLee\n55000\n\n\n4\nDavid\nKim\n75000\n\n\n5\nEva\nBrown\n65000\n\n\n6\nFrank\nWilson\n70000\n\n\n7\nGrace\nTaylor\n62000\n\n\n8\nHenry\nAnderson\n77000\n\n\n9\nIrene\nThomas\n53000\n\n\n10\nJack\nWhite\n58000\n\n\n11\nKaren\nHarris\n69000\n\n\n12\nLeo\nMartin\n50000\n\n\n13\nMia\nJackson\n64000\n\n\n14\nNoah\nLee\n72000\n\n\n15\nOlivia\nPerez\n68000\n\n\n16\nPaul\nYoung\n61000\n\n\n17\nQuinn\nKing\n76000\n\n\n18\nRachel\nScott\n57000\n\n\n19\nSam\nGreen\n63000\n\n\n20\nTina\nAdams\n81000\n\n\n\n\n\nSELECT EXCLUDE and RENAME\nYou can exclude columns using SELECT and rename them for clarity.\n-- Selecting all but one column\np = \"\"\"\n\nSELECT * \nEXCLUDE \n  salary \nFROM \n  employees;\n\n\"\"\"\n\npd.read_sql_query(p, con)\n\n\n\nid\nfirst_name\nlast_name\ndepartment\n\n\n\n\n1\nAlice\nSmith\nHR\n\n\n2\nBob\nJohnson\nIT\n\n\n3\nCharlie\nLee\nSales\n\n\n4\nDavid\nKim\nHR\n\n\n5\nEva\nBrown\nIT\n\n\n6\nFrank\nWilson\nSales\n\n\n7\nGrace\nTaylor\nHR\n\n\n8\nHenry\nAnderson\nIT\n\n\n9\nIrene\nThomas\nSales\n\n\n10\nJack\nWhite\nHR\n\n\n11\nKaren\nHarris\nIT\n\n\n12\nLeo\nMartin\nSales\n\n\n13\nMia\nJackson\nHR\n\n\n14\nNoah\nLee\nIT\n\n\n15\nOlivia\nPerez\nSales\n\n\n16\nPaul\nYoung\nHR\n\n\n17\nQuinn\nKing\nIT\n\n\n18\nRachel\nScott\nSales\n\n\n19\nSam\nGreen\nHR\n\n\n20\nTina\nAdams\nIT\n\n\n\n-- Renaming columns\np = \"\"\"\n\nSELECT \n  first_name AS fname, \n  last_name AS lname \nFROM \n  employees;\n\n\"\"\"\n\npd.read_sql_query(p, con)\n\n\n\nfname\nlname\n\n\n\n\nAlice\nSmith\n\n\nBob\nJohnson\n\n\nCharlie\nLee\n\n\nDavid\nKim\n\n\nEva\nBrown\n\n\nFrank\nWilson\n\n\nGrace\nTaylor\n\n\nHenry\nAnderson\n\n\nIrene\nThomas\n\n\nJack\nWhite\n\n\nKaren\nHarris\n\n\nLeo\nMartin\n\n\nMia\nJackson\n\n\nNoah\nLee\n\n\nOlivia\nPerez\n\n\nPaul\nYoung\n\n\nQuinn\nKing\n\n\nRachel\nScott\n\n\nSam\nGreen\n\n\nTina\nAdams\n\n\n\n\n\nLIMIT and OFFSET\nThe LIMIT clause restricts the number of rows returned, and OFFSET skips rows before beginning to return rows.\n-- Limiting the number of rows returned\n\np = \"\"\"\n\nSELECT * \nFROM \n  employees \nLIMIT \n  10;\n\n\"\"\"\n\npd.read_sql_query(p, con)\n\n\n\nid\nfirst_name\nlast_name\ndepartment\nsalary\n\n\n\n\n1\nAlice\nSmith\nHR\n60000\n\n\n2\nBob\nJohnson\nIT\n80000\n\n\n3\nCharlie\nLee\nSales\n55000\n\n\n4\nDavid\nKim\nHR\n75000\n\n\n5\nEva\nBrown\nIT\n65000\n\n\n6\nFrank\nWilson\nSales\n70000\n\n\n7\nGrace\nTaylor\nHR\n62000\n\n\n8\nHenry\nAnderson\nIT\n77000\n\n\n9\nIrene\nThomas\nSales\n53000\n\n\n10\nJack\nWhite\nHR\n58000\n\n\n\n-- Skipping rows\np = \"\"\"\n\nSELECT * \nFROM \n  employees \nLIMIT 10 \nOFFSET 5;\n\n\"\"\"\n\npd.read_sql_query(p, con)\n\n\n\nid\nfirst_name\nlast_name\ndepartment\nsalary\n\n\n\n\n6\nFrank\nWilson\nSales\n70000\n\n\n7\nGrace\nTaylor\nHR\n62000\n\n\n8\nHenry\nAnderson\nIT\n77000\n\n\n9\nIrene\nThomas\nSales\n53000\n\n\n10\nJack\nWhite\nHR\n58000\n\n\n11\nKaren\nHarris\nIT\n69000\n\n\n12\nLeo\nMartin\nSales\n50000\n\n\n13\nMia\nJackson\nHR\n64000\n\n\n14\nNoah\nLee\nIT\n72000\n\n\n15\nOlivia\nPerez\nSales\n68000\n\n\n\n\n\nORDER BY\nThe ORDER BY clause sorts the result set.\n-- Sorting the result set by salary in ascending order\np = \"\"\"\n\nSELECT * \nFROM  \n  employees \nORDER BY \n  salary;\n\n\"\"\"\n\npd.read_sql_query(p, con)\n\n\n\nid\nfirst_name\nlast_name\ndepartment\nsalary\n\n\n\n\n12\nLeo\nMartin\nSales\n50000\n\n\n9\nIrene\nThomas\nSales\n53000\n\n\n3\nCharlie\nLee\nSales\n55000\n\n\n10\nJack\nWhite\nHR\n58000\n\n\n1\nAlice\nSmith\nHR\n60000\n\n\n19\nSam\nGreen\nHR\n63000\n\n\n7\nGrace\nTaylor\nHR\n62000\n\n\n13\nMia\nJackson\nHR\n64000\n\n\n5\nEva\nBrown\nIT\n65000\n\n\n11\nKaren\nHarris\nIT\n69000\n\n\n18\nRachel\nScott\nSales\n57000\n\n\n15\nOlivia\nPerez\nSales\n68000\n\n\n6\nFrank\nWilson\nSales\n70000\n\n\n14\nNoah\nLee\nIT\n72000\n\n\n17\nQuinn\nKing\nIT\n76000\n\n\n8\nHenry\nAnderson\nIT\n77000\n\n\n4\nDavid\nKim\nHR\n75000\n\n\n2\nBob\nJohnson\nIT\n80000\n\n\n20\nTina\nAdams\nIT\n81000\n\n\n16\nPaul\nYoung\nHR\n61000\n\n\n\n-- Sorting in descending order\np = \"\"\"\n\nSELECT * \nFROM \n  employees \nORDER BY \n  salary DESC;\n\n\"\"\"\n\npd.read_sql_query(p, con)\n\n\n\nid\nfirst_name\nlast_name\ndepartment\nsalary\n\n\n\n\n20\nTina\nAdams\nIT\n81000\n\n\n2\nBob\nJohnson\nIT\n80000\n\n\n8\nHenry\nAnderson\nIT\n77000\n\n\n17\nQuinn\nKing\nIT\n76000\n\n\n4\nDavid\nKim\nHR\n75000\n\n\n14\nNoah\nLee\nIT\n72000\n\n\n6\nFrank\nWilson\nSales\n70000\n\n\n15\nOlivia\nPerez\nSales\n68000\n\n\n11\nKaren\nHarris\nIT\n69000\n\n\n5\nEva\nBrown\nIT\n65000\n\n\n13\nMia\nJackson\nHR\n64000\n\n\n19\nSam\nGreen\nHR\n63000\n\n\n7\nGrace\nTaylor\nHR\n62000\n\n\n16\nPaul\nYoung\nHR\n61000\n\n\n1\nAlice\nSmith\nHR\n60000\n\n\n10\nJack\nWhite\nHR\n58000\n\n\n3\nCharlie\nLee\nSales\n55000\n\n\n9\nIrene\nThomas\nSales\n53000\n\n\n18\nRachel\nScott\nSales\n57000\n\n\n12\nLeo\nMartin\nSales\n50000\n\n\n\n\n\nAND, OR, NOT\nLogical operators filter records based on multiple conditions.\n-- Using AND, OR, NOT operators\np = \"\"\"\n\nSELECT * \nFROM \n  employees \nWHERE \n  department = 'Sales' AND salary &gt; 50000;\n\n\"\"\"\n\npd.read_sql_query(p, con)\n\n\n\nid\nfirst_name\nlast_name\ndepartment\nsalary\n\n\n\n\n6\nFrank\nWilson\nSales\n70000\n\n\n15\nOlivia\nPerez\nSales\n68000\n\n\n18\nRachel\nScott\nSales\n57000\n\n\n\n\n\nNumeric Operations\nPerform arithmetic operations in SQL.\n-- Calculating a new column\np = \"\"\"\n\nSELECT \n  first_name, \n  last_name, \n  salary, \n  salary * 1.1 AS new_salary \nFROM \n  employees;\n\"\"\"\n\npd.read_sql_query(p, con)\n\n\n\nfirst_name\nlast_name\nsalary\nnew_salary\n\n\n\n\nAlice\nSmith\n60000\n66000.0\n\n\nBob\nJohnson\n80000\n88000.0\n\n\nCharlie\nLee\n55000\n60500.0\n\n\nDavid\nKim\n75000\n82500.0\n\n\nEva\nBrown\n65000\n71500.0\n\n\nFrank\nWilson\n70000\n77000.0\n\n\nGrace\nTaylor\n62000\n68200.0\n\n\nHenry\nAnderson\n77000\n84700.0\n\n\nIrene\nThomas\n53000\n58300.0\n\n\nJack\nWhite\n58000\n63800.0\n\n\nKaren\nHarris\n69000\n75900.0\n\n\nLeo\nMartin\n50000\n55000.0\n\n\nMia\nJackson\n64000\n70400.0\n\n\nNoah\nLee\n72000\n79200.0\n\n\nOlivia\nPerez\n68000\n74800.0\n\n\nPaul\nYoung\n61000\n67100.0\n\n\nQuinn\nKing\n76000\n83600.0\n\n\nRachel\nScott\n57000\n62700.0\n\n\nSam\nGreen\n63000\n69300.0\n\n\nTina\nAdams\n81000\n89100.0\n\n\n\n\n\nLIKE and NOT LIKE\nPattern matching using LIKE.\n-- Pattern matching\np = \"\"\"\n\nSELECT * \nFROM \n  employees \nWHERE \n  last_name \nLIKE 'S%';\n\n\"\"\"\n\npd.read_sql_query(p, con)\n\n\n\nid\nfirst_name\nlast_name\ndepartment\nsalary\n\n\n\n\n1\nAlice\nSmith\nHR\n60000\n\n\n18\nRachel\nScott\nSales\n57000\n\n\n\n\n\nBETWEEN\nRange filtering using BETWEEN.\n-- Filtering within a range\np = \"\"\"\n\nSELECT * \nFROM \n  employees \nWHERE \n  salary BETWEEN 40000 AND 60000;\n\n\"\"\"\n\npd.read_sql_query(p, con)\n\n\n\nid\nfirst_name\nlast_name\ndepartment\nsalary\n\n\n\n\n12\nLeo\nMartin\nSales\n50000\n\n\n3\nCharlie\nLee\nSales\n55000\n\n\n9\nIrene\nThomas\nSales\n53000\n\n\n10\nJack\nWhite\nHR\n58000\n\n\n1\nAlice\nSmith\nHR\n60000\n\n\n\n\n\nOFFSET\nSkip a specific number of rows before starting to return rows.\n-- Skipping the first 5 rows\np = \"\"\"\n\nSELECT * \nFROM \n  employees \nOFFSET 5;\n\n\"\"\"\n\npd.read_sql_query(p, con)\n\n\n\nid\nfirst_name\nlast_name\ndepartment\nsalary\n\n\n\n\n6\nFrank\nWilson\nSales\n70000\n\n\n7\nGrace\nTaylor\nHR\n62000\n\n\n8\nHenry\nAnderson\nIT\n77000\n\n\n9\nIrene\nThomas\nSales\n53000\n\n\n10\nJack\nWhite\nHR\n58000\n\n\n11\nKaren\nHarris\nIT\n69000\n\n\n12\nLeo\nMartin\nSales\n50000\n\n\n13\nMia\nJackson\nHR\n64000\n\n\n14\nNoah\nLee\nIT\n72000\n\n\n15\nOlivia\nPerez\nSales\n68000\n\n\n16\nPaul\nYoung\nHR\n61000\n\n\n17\nQuinn\nKing\nIT\n76000\n\n\n18\nRachel\nScott\nSales\n57000\n\n\n19\nSam\nGreen\nHR\n63000\n\n\n20\nTina\nAdams\nIT\n81000"
  },
  {
    "objectID": "Course Materials/sql_page.html#intermediate-sql-concepts",
    "href": "Course Materials/sql_page.html#intermediate-sql-concepts",
    "title": "Introduction",
    "section": "Intermediate SQL Concepts",
    "text": "Intermediate SQL Concepts\n\nJoins\nTable: employees\n\n\n\nid\nfirst_name\nlast_name\ndepartment_id\nsalary\n\n\n\n\n1\nAlice\nSmith\n1\n60000\n\n\n2\nBob\nJohnson\n2\n80000\n\n\n3\nCharlie\nLee\n3\n55000\n\n\n4\nDavid\nKim\n1\n75000\n\n\n5\nEva\nBrown\n2\n65000\n\n\n6\nFrank\nWilson\n3\n70000\n\n\n7\nGrace\nTaylor\n1\n62000\n\n\n8\nHenry\nAnderson\n2\n77000\n\n\n9\nIrene\nThomas\n3\n53000\n\n\n10\nJack\nWhite\n1\n58000\n\n\n11\nKaren\nHarris\n2\n69000\n\n\n12\nLeo\nMartin\n3\n50000\n\n\n13\nMia\nJackson\n1\n64000\n\n\n14\nNoah\nLee\n2\n72000\n\n\n15\nOlivia\nPerez\n3\n68000\n\n\n16\nPaul\nYoung\n1\n61000\n\n\n17\nQuinn\nKing\n2\n76000\n\n\n18\nRachel\nScott\n3\n57000\n\n\n19\nSam\nGreen\n1\n63000\n\n\n20\nTina\nAdams\n2\n81000\n\n\n\nTable: departments\n\n\n\ndepartment_id\ndepartment_name\n\n\n\n\n1\nHR\n\n\n2\nIT\n\n\n3\nSales\n\n\n\nCombine rows from two or more tables based on a related column.\n-- Inner join example\np = \"\"\"\n\nSELECT \n  employees.first_name, \n    employees.last_name, \n    departments.department_name\nFROM \n  employees\nINNER JOIN \n  departments ON employees.department_id = departments.department_id;\n\n\"\"\"\n\npd.read_sql_query(p, con)\n\n\n\nfirst_name\nlast_name\ndepartment_name\n\n\n\n\nAlice\nSmith\nHR\n\n\nDavid\nKim\nHR\n\n\nGrace\nTaylor\nHR\n\n\nJack\nWhite\nHR\n\n\nMia\nJackson\nHR\n\n\nPaul\nYoung\nHR\n\n\nSam\nGreen\nHR\n\n\nBob\nJohnson\nIT\n\n\nEva\nBrown\nIT\n\n\nHenry\nAnderson\nIT\n\n\nKaren\nHarris\nIT\n\n\nNoah\nLee\nIT\n\n\nQuinn\nKing\nIT\n\n\nTina\nAdams\nIT\n\n\nCharlie\nLee\nSales\n\n\nFrank\nWilson\nSales\n\n\nIrene\nThomas\nSales\n\n\nLeo\nMartin\nSales\n\n\nOlivia\nPerez\nSales\n\n\nRachel\nScott\nSales\n\n\n\n\n\nCAST\nConvert data from one type to another.\n-- Casting a column\np = \"\"\"\n\nSELECT \n  CAST(salary AS DECIMAL(10, 2)) \nFROM \n  employees;\n\n\"\"\"\n\npd.read_sql_query(p, con)\n\n\n\nsalary\n\n\n\n\n60000.00\n\n\n80000.00\n\n\n55000.00\n\n\n75000.00\n\n\n65000.00\n\n\n70000.00\n\n\n62000.00\n\n\n77000.00\n\n\n53000.00\n\n\n58000.00\n\n\n69000.00\n\n\n50000.00\n\n\n64000.00\n\n\n72000.00\n\n\n68000.00\n\n\n61000.00\n\n\n76000.00\n\n\n57000.00\n\n\n63000.00\n\n\n81000.00\n\n\n\n\n\nAggregations\nPerform calculations on a set of values.\n-- Using aggregation functions\np = \"\"\"\n\nSELECT \n  department_id, \n  COUNT(employee_id) AS num_employees\nFROM \n  employees\nGROUP BY \n  department_id;\n\n\"\"\"\n\npd.read_sql_query(p, con)\n\n\n\ndepartment_id\nnum_employees\n\n\n\n\n1\n7\n\n\n2\n7\n\n\n3\n6\n\n\n\n\n\nGROUP BY and HAVING\nGroup rows that have the same values and filter groups.\n-- Grouping rows and filtering groups\np = \"\"\"\nSELECT \n  department_id,\n  COUNT(employee_id) AS num_employees\nFROM \n  employees\nGROUP BY \n  department_id\nHAVING COUNT(employee_id) &gt; 5;\n\"\"\"\n\npd.read_sql_query(p, con)\n\n\n\ndepartment_id\nnum_employees\n\n\n\n\n1\n7\n\n\n2\n7\n\n\n3\n6\n\n\n\n\n\nUNION, INTERSECT, MINUS\nCombine result sets\nTable: managers\n\n\n\nmanager_id\nfirst_name\nlast_name\n\n\n\n\n1\nMichael\nBrown\n\n\n2\nSarah\nJohnson\n\n\n3\nJohn\nLee\n\n\n\n-- Union example\np = \"\"\"\nSELECT \n  first_name \nFROM \n  employees\nUNION\n\nSELECT \n  first_name\nFROM\n   managers;\n\"\"\"\n\npd.read_sql_query(p, con)\n\n\n\nfirst_name\n\n\n\n\nAlice\n\n\nBob\n\n\nCharlie\n\n\nDavid\n\n\nEva\n\n\nFrank\n\n\nGrace\n\n\nHenry\n\n\nIrene\n\n\nJack\n\n\nKaren\n\n\nLeo\n\n\nMia\n\n\nNoah\n\n\nOlivia\n\n\nPaul\n\n\nQuinn\n\n\nRachel\n\n\nSam\n\n\nTina\n\n\nMichael\n\n\nSarah\n\n\nJohn\n\n\n\n\n\nPOSITION\nFind the position of a substring.\n-- Finding substring position\np = \"\"\"\nSELECT \n  POSITION('e' IN first_name) \nFROM \n  employees;\n\"\"\"\n\npd.read_sql_query(p, con)\n\n\n\nposition\n\n\n\n\n0\n\n\n0\n\n\n4\n\n\n0\n\n\n0\n\n\n2\n\n\n0\n\n\n3\n\n\n2\n\n\n0\n\n\n0\n\n\n3\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n2\n\n\n0\n\n\n0\n\n\n\n\n\nCASE\nConditional logic in `SQL.\n-- Using CASE statements\np = \"\"\"\nSELECT \n  first_name, \n  last_name,\n       CASE \n         WHEN salary &gt; 60000 THEN 'High'\n         WHEN salary BETWEEN 40000 AND 60000 THEN 'Medium'\n         ELSE 'Low'\n       END AS salary_category\nFROM \n  employees;\n\"\"\"\n\npd.read_sql_query(p, con)\n\n\n\nfirst_name\nlast_name\nsalary_category\n\n\n\n\nAlice\nSmith\nMedium\n\n\nBob\nJohnson\nHigh\n\n\nCharlie\nLee\nMedium\n\n\nDavid\nKim\nHigh\n\n\nEva\nBrown\nMedium\n\n\nFrank\nWilson\nHigh\n\n\nGrace\nTaylor\nMedium\n\n\nHenry\nAnderson\nHigh\n\n\nIrene\nThomas\nMedium\n\n\nJack\nWhite\nMedium\n\n\nKaren\nHarris\nHigh\n\n\nLeo\nMartin\nMedium\n\n\nMia\nJackson\nMedium\n\n\nNoah\nLee\nHigh\n\n\nOlivia\nPerez\nHigh\n\n\nPaul\nYoung\nMedium\n\n\nQuinn\nKing\nHigh\n\n\nRachel\nScott\nMedium\n\n\nSam\nGreen\nMedium\n\n\nTina\nAdams\nHigh"
  },
  {
    "objectID": "Course Materials/sql_page.html#introduction",
    "href": "Course Materials/sql_page.html#introduction",
    "title": "Introduction",
    "section": "Introduction",
    "text": "Introduction\nIt’s rare that a data analysis involves only a single data frame. Typically you have many data frames, and you must join them together to answer the questions that you’re interested in.\npandas has a really rich set of options for combining one or more data frames, with the two most important being concatenate and merge. Some of the examples in this chapter show you how to join a pair of data frames. Fortunately this is enough, since you can combine three data frames by combining two pairs.\n\n# remove cell\nimport matplotlib_inline.backend_inline\nimport matplotlib.pyplot as plt\n\n# Plot settings\nplt.style.use(\"https://github.com/aeturrell/python4DS/raw/main/plot_style.txt\")\nmatplotlib_inline.backend_inline.set_matplotlib_formats(\"svg\")\n\n\nPrerequisites\nThis chapter will use the pandas data analysis package."
  },
  {
    "objectID": "Course Materials/sql_page.html#concatenate",
    "href": "Course Materials/sql_page.html#concatenate",
    "title": "Introduction",
    "section": "Concatenate",
    "text": "Concatenate\nIf you have two or more data frames with the same index or the same columns, you can glue them together into a single data frame using pd.concat().\n\nFor the same columns, pass axis=0 to glue the index together; for the same index, pass axis=1 to glue the columns together. The concatenate function will typically be used on a list of data frames.\nIf you want to track where the original data came from in the final data frame, use the keys keyword.\nHere’s an example using data on two different states’ populations that also makes uses of the keys option:\n\nimport pandas as pd\nimport urllib.request\n\nbase_url = \"http://www.stata-press.com/data/r14/\"\nstate_codes = [\"ca\", \"il\"]\nend_url = \"pop.dta\"\nheaders = {'User-Agent': 'Mozilla/5.0'}\n\ndef fetch_data(url):\n    req = urllib.request.Request(url, headers=headers)\n    with urllib.request.urlopen(req) as response:\n        return pd.read_stata(response)\n\n# This grabs the two data frames, one for each state\nlist_of_state_dfs = [fetch_data(base_url + state + end_url) for state in state_codes]\n\n# Show example of first entry in list of data frames\nprint(list_of_state_dfs[0])\n\n        county      pop\n0  Los Angeles  9878554\n1       Orange  2997033\n2      Ventura   798364\n\n\n\n# Concatenate the list of data frames\ndf = pd.concat(list_of_state_dfs, keys=state_codes, axis=0)\n\nNote that the keys argument is optional, but is useful for keeping track of origin data frames within the merged data frame.\n\nExercise\nConcatenate the follow two data frames:\ndf1 = pd.DataFrame([['a', 1], ['b', 2]],\n                   columns=['letter', 'number'])\n\ndf2 = pd.DataFrame([['c', 3], ['d', 4]],\n                   columns=['letter', 'number'])\n\n\nMerge\nThere are so many options for merging data frames using pd.merge(left, right, on=..., how=... that we won’t be able to cover them all here. The most important features are: the two data frames to be merged, what variables (aka keys) to merge on (and these can be indexes) via on=, and how to do the merge (eg left, right, outer, inner) via how=. This diagram shows an example of a merge using keys from the left-hand data frame:\n\nThe how= keyword works in the following ways: - how='left' uses keys from the left data frame only to merge. - how='right' uses keys from the right data frame only to merge. - how='inner' uses keys that appear in both data frames to merge. - how='outer' uses the cartesian product of keys in both data frames to merge on.\nLet’s see examples of some of these:\n\nleft = pd.DataFrame(\n    {\n        \"key1\": [\"K0\", \"K0\", \"K1\", \"K2\"],\n        \"key2\": [\"K0\", \"K1\", \"K0\", \"K1\"],\n        \"A\": [\"A0\", \"A1\", \"A2\", \"A3\"],\n        \"B\": [\"B0\", \"B1\", \"B2\", \"B3\"],\n    }\n)\nright = pd.DataFrame(\n    {\n        \"key1\": [\"K0\", \"K1\", \"K1\", \"K2\"],\n        \"key2\": [\"K0\", \"K0\", \"K0\", \"K0\"],\n        \"C\": [\"C0\", \"C1\", \"C2\", \"C3\"],\n        \"D\": [\"D0\", \"D1\", \"D2\", \"D3\"],\n    }\n)\n# Right merge\npd.merge(left, right, on=[\"key1\", \"key2\"], how=\"right\")\n\n\n\n\n\n\n\n\nkey1\nkey2\nA\nB\nC\nD\n\n\n\n\n0\nK0\nK0\nA0\nB0\nC0\nD0\n\n\n1\nK1\nK0\nA2\nB2\nC1\nD1\n\n\n2\nK1\nK0\nA2\nB2\nC2\nD2\n\n\n3\nK2\nK0\nNaN\nNaN\nC3\nD3\n\n\n\n\n\n\n\nNote that the key combination of K2 and K0 did not exist in the left-hand data frame, and so its entries in the final data frame are NaNs. But it does have entries because we chose the keys from the right-hand data frame.\nWhat about an inner merge?\n\npd.merge(left, right, on=[\"key1\", \"key2\"], how=\"inner\")\n\n\n\n\n\n\n\n\nkey1\nkey2\nA\nB\nC\nD\n\n\n\n\n0\nK0\nK0\nA0\nB0\nC0\nD0\n\n\n1\nK1\nK0\nA2\nB2\nC1\nD1\n\n\n2\nK1\nK0\nA2\nB2\nC2\nD2\n\n\n\n\n\n\n\nNow we see that the combination K2 and K0 are excluded because they didn’t exist in the overlap of keys in both data frames.\nFinally, let’s take a look at an outer merge that comes with some extra info via the indicator keyword:\n\npd.merge(left, right, on=[\"key1\", \"key2\"], how=\"outer\", indicator=True)\n\n\n\n\n\n\n\n\nkey1\nkey2\nA\nB\nC\nD\n_merge\n\n\n\n\n0\nK0\nK0\nA0\nB0\nC0\nD0\nboth\n\n\n1\nK0\nK1\nA1\nB1\nNaN\nNaN\nleft_only\n\n\n2\nK1\nK0\nA2\nB2\nC1\nD1\nboth\n\n\n3\nK1\nK0\nA2\nB2\nC2\nD2\nboth\n\n\n4\nK2\nK0\nNaN\nNaN\nC3\nD3\nright_only\n\n\n5\nK2\nK1\nA3\nB3\nNaN\nNaN\nleft_only\n\n\n\n\n\n\n\nNow we can see that the products of all key combinations are here. The indicator=True option has caused an extra column to be added, called ’_merge’, that tells us which data frame the keys on that row came from.\n\n\nExercise\nMerge the following two data frames using the left_on and right_on keyword arguments to specify a join on lkey and rkey respectively:\ndf1 = pd.DataFrame({'lkey': ['foo', 'bar', 'baz', 'foo'],\n                    'value': [1, 2, 3, 5]})\ndf2 = pd.DataFrame({'rkey': ['foo', 'bar', 'baz', 'foo'],\n                    'value': [5, 6, 7, 8]})\n\n\nExercise\nMerge the following two data frames on \"a\" using how=\"left\" as a keyword argument:\ndf1 = pd.DataFrame({'a': ['foo', 'bar'], 'b': [1, 2]})\ndf2 = pd.DataFrame({'a': ['foo', 'baz'], 'c': [3, 4]})\nWhat do you notice about the position .loc[1, \"c\"] in the merged data frame?\nFor more on the options for merging, see pandas’ comprehensive merging documentation."
  },
  {
    "objectID": "Course Materials/sql_page.html#introduction-to-sql-joins",
    "href": "Course Materials/sql_page.html#introduction-to-sql-joins",
    "title": "Introduction",
    "section": "Introduction to SQL Joins",
    "text": "Introduction to SQL Joins\nSQL joins are used to combine rows from two or more tables based on a related column between them. Understanding joins is crucial for effective data retrieval and manipulation. This section will cover the four main types of joins: INNER JOIN, LEFT JOIN, RIGHT JOIN, and FULL JOIN, using visual aids and code snippets with a fake data frame."
  },
  {
    "objectID": "Course Materials/sql_page.html#types-of-joins",
    "href": "Course Materials/sql_page.html#types-of-joins",
    "title": "Introduction",
    "section": "Types of Joins",
    "text": "Types of Joins\n\nINNER JOIN\nAn INNER JOIN returns only the rows that have matching values in both tables.\n\n\n\nINNER JOIN\n\n\n-- Inner join example\np = \"\"\"\nSELECT \n  A.id, \n  A.name, \n  B.order_id\nFROM \n  Customers A\nINNER JOIN \n  Orders B ON A.id = B.customer_id;\n\"\"\"\npd.read_sql_query(p, con)\n\n\nLEFT JOIN\nA LEFT JOIN returns all the rows from the left table and the matched rows from the right table. If no match is found, NULL values are returned for columns from the right table.\n\n\n\nLEFT JOIN\n\n\n-- Left join example\np = \"\"\"\nSELECT \n  A.id, \n  A.name, \n  B.order_id\nFROM \n  Customers A\nLEFT JOIN \n  Orders B ON A.id = B.customer_id;\n\"\"\"\npd.read_sql_query(p, con)\n\n\nRIGHT JOIN\nA RIGHT JOIN returns all the rows from the right table and the matched rows from the left table. If no match is found, NULL values are returned for columns from the left table.\n\n\n\nRIGHT JOIN\n\n\np = \"\"\"\nSELECT \n  A.id, \n  A.name, \n  B.order_id\nFROM \n  Customers A\nRIGHT JOIN \n  Orders B ON A.id = B.customer_id;\n\"\"\"\npd.read_sql_query(p, con)\n\n\nFULL JOIN\nA FULL JOIN returns all the rows when there is a match in either left or right table. Rows without a match in one of the tables will contain NULL values for columns from that table.\n\n\n\nFULL JOIN\n\n\n-- Full join example\np = \"\"\"\nSELECT \n  A.id, \n  A.name, \n  B.order_id\nFROM \n  Customers A\nFULL JOIN \n  Orders B ON A.id = B.customer_id;\n\"\"\"\npd.read_sql_query(p, con)"
  },
  {
    "objectID": "Course Materials/sql_page.html#example-data-frames",
    "href": "Course Materials/sql_page.html#example-data-frames",
    "title": "Introduction",
    "section": "Example Data Frames",
    "text": "Example Data Frames\nTo illustrate these joins, let’s consider two fake data frames: Customers and Orders.\n\nimport pandas as pd\n\n# Creating a fake data frame for Customers\ncustomers = pd.DataFrame({\n    'id': [1, 2, 3, 4],\n    'name': ['Alice', 'Bob', 'Charlie', 'David']\n})\n\n# Creating a fake data frame for Orders\norders = pd.DataFrame({\n    'order_id': [101, 102, 103, 104],\n    'customer_id': [1, 2, 2, 4]\n})\n\n# Display the data frames\nprint(\"Customers Data Frame\")\nprint(customers)\n\nCustomers Data Frame\n   id     name\n0   1    Alice\n1   2      Bob\n2   3  Charlie\n3   4    David\n\n\n\nprint(\"Orders Data Frame\")\nprint(orders)\n\nOrders Data Frame\n   order_id  customer_id\n0       101            1\n1       102            2\n2       103            2\n3       104            4"
  },
  {
    "objectID": "Course Materials/sql_page.html#conclusion",
    "href": "Course Materials/sql_page.html#conclusion",
    "title": "Introduction",
    "section": "Conclusion",
    "text": "Conclusion\nSQL is an indispensable tool for anyone working with data. Its ability to manage, manipulate, and integrate data makes it essential for data analysis and decision-making. The examples provided in this chapter illustrate some of the key operations and techniques used in SQL, highlighting its importance in the field of data science."
  }
]